{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc9067-e714-46e0-bb69-e5156f8607eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install stuff\n",
    "# IMPORTANT NOTE: before starting Jupyter Notebook, navigate to /home/peter/Desktop/yolov5. Do source .venv/bin/activate\n",
    "# this sets up the virtual environment where torch is installed and allows yolo to work.\n",
    "# if installing this fresh, you'll need to install torch with Cuda (or not depending on the hardware)\n",
    "#Yolov5\n",
    "%cd /home/peter/Desktop/yolov5\n",
    "! source .venv/bin/activate\n",
    "!uv pip install seaborn\n",
    "!uv pip install gitpython>=3.1.30\n",
    "!uv pip install matplotlib>=3.3\n",
    "!uv pip install numpy>=1.23.5\n",
    "!uv pip install opencv-python>=4.1.1\n",
    "!uv pip install pillow>=10.3.0\n",
    "!uv pip install psutil  # system resources\n",
    "!uv pip install PyYAML>=5.3.1\n",
    "!uv pip install requests>=2.32.2\n",
    "!uv pip install scipy>=1.4.1\n",
    "!uv pip install hop>=0.1.1  # FLOPs computation\n",
    "!uv pip install torch>=1.8.0  # see https://pytorch.org/get-started/locally (recommended)\n",
    "!uv pip install torchvision>=0.9.0\n",
    "!uv pip install tqdm>=4.66.3\n",
    "!uv pip install ultralytics>=8.2.64  # https://ultralytics.com\n",
    "!uv pip install pandas>=1.1.4\n",
    "!uv pip install seaborn>=0.11.0\n",
    "# assume the files are in the right places. See https://github.com/ch-hristov/p-id-symbols for details. Follow the notebook\n",
    "#Ollama\n",
    "!uv pip install ollama\n",
    "#Claude\n",
    "!uv pip install anthropic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ecb1899-ec00-4726-9547-2c513502f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yolo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import ast\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import platform\n",
    "import sys\n",
    "# import torch ***this one is trouble\n",
    "\n",
    "#ollama\n",
    "import ollama\n",
    "import base64\n",
    "# import os duplicate\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "#Claude\n",
    "import anthropic\n",
    "\n",
    "#image handling\n",
    "import os\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import fitz  # PyMuPDF\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63268694-1e2f-4c93-a848-0f81539af88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anthropic key - do not publish\n",
    "key=\"sk-ant-api03-cXredactedsswAA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d5810a-49f6-46d1-ba7e-e9d7d0695fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split P&ID into 4 quadrants\n",
    "\n",
    "def pdf_to_images(pdf_path, page_number=0, dpi=300):\n",
    "    \"\"\"\n",
    "    Convert a PDF page to a high-quality PIL Image\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        page_number (int): Page number to convert (0-indexed)\n",
    "        dpi (int): Resolution for the conversion\n",
    "    \n",
    "    Returns:\n",
    "        PIL.Image: The converted page as an image\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    if page_number >= len(doc):\n",
    "        raise ValueError(f\"Page {page_number} doesn't exist. PDF has {len(doc)} pages.\")\n",
    "    \n",
    "    page = doc[page_number]\n",
    "    \n",
    "    # Create a transformation matrix for higher resolution\n",
    "    mat = fitz.Matrix(dpi/72, dpi/72)  # 72 is the default DPI\n",
    "    \n",
    "    # Render the page as a pixmap\n",
    "    pix = page.get_pixmap(matrix=mat)\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    img_data = pix.tobytes(\"png\")\n",
    "    image = Image.open(io.BytesIO(img_data))\n",
    "    \n",
    "    doc.close()\n",
    "    return image\n",
    "\n",
    "def split_image_into_quadrants(image):\n",
    "    \"\"\"\n",
    "    Split a PIL Image into 4 equal quadrants\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image): The source image\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Four PIL Images representing the quadrants (top-left, top-right, bottom-left, bottom-right)\n",
    "    \"\"\"\n",
    "    width, height = image.size\n",
    "    \n",
    "    # Calculate quadrant dimensions\n",
    "    half_width = width // 2\n",
    "    half_height = height // 2\n",
    "    \n",
    "    # Define crop boxes for each quadrant\n",
    "    # (left, top, right, bottom)\n",
    "    top_left = (0, 0, half_width, half_height)\n",
    "    top_right = (half_width, 0, width, half_height)\n",
    "    bottom_left = (0, half_height, half_width, height)\n",
    "    bottom_right = (half_width, half_height, width, height)\n",
    "    \n",
    "    # Crop the image into quadrants\n",
    "    quadrants = (\n",
    "        image.crop(top_left),\n",
    "        image.crop(top_right),\n",
    "        image.crop(bottom_left),\n",
    "        image.crop(bottom_right)\n",
    "    )\n",
    "    \n",
    "    return quadrants\n",
    "\n",
    "def save_quadrants_as_png(quadrants, output_dir, base_filename):\n",
    "    \"\"\"\n",
    "    Save quadrant images as PNG files\n",
    "    \n",
    "    Args:\n",
    "        quadrants (tuple): Four PIL Images\n",
    "        output_dir (str): Directory to save the images\n",
    "        base_filename (str): Base name for the output files\n",
    "    \n",
    "    Returns:\n",
    "        list: Paths to the saved PNG files\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    quadrant_names = [\"top_left\", \"top_right\", \"bottom_left\", \"bottom_right\"]\n",
    "    saved_paths = []\n",
    "    \n",
    "    for i, (quadrant, name) in enumerate(zip(quadrants, quadrant_names)):\n",
    "        filename = f\"{base_filename}_q{i+1}.png\"\n",
    "#        filename = f\"{base_filename}_quadrant_{i+1}_{name}.png\"\n",
    "        filepath = output_dir / filename\n",
    "        \n",
    "        # Save as PNG with high quality\n",
    "        quadrant.save(filepath, \"PNG\", optimize=True)\n",
    "        saved_paths.append(str(filepath))\n",
    "#        print(f\"Saved quadrant {i+1} ({name}): {filepath}\")\n",
    "    \n",
    "    return saved_paths\n",
    "    \n",
    "def create_quads(pdf_path,page,output_dir):\n",
    "     image = pdf_to_images(pdf_path, page)\n",
    "     quadrants = split_image_into_quadrants(image)\n",
    "# Save quadrants as PNG files\n",
    "     base_filename = Path(pdf_path).stem\n",
    "     quadrant_paths = save_quadrants_as_png(quadrants, output_dir, base_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f604a422-9323-439d-a9f7-bbf0047b5969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call Ollama LLMs. Encode PNGs too.\n",
    "def encode_png_file(png_path: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Encode a PNG file to base64\n",
    "    \n",
    "    Args:\n",
    "        png_path (str): Path to the PNG file\n",
    "    \n",
    "    Returns:\n",
    "        str: Base64 encoded image data, or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(png_path, 'rb') as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding {png_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_png_files(directory: str, pattern: str = \"*.png\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Find all PNG files in a directory\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Directory to search\n",
    "        pattern (str): File pattern (default: \"*.png\")\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: List of PNG file paths\n",
    "    \"\"\"\n",
    "    search_path = os.path.join(directory, pattern)\n",
    "    png_files = glob.glob(search_path)\n",
    "    png_files.sort()  # Sort for consistent ordering\n",
    "    return png_files\n",
    "\n",
    "def query_ollama(model, prompt, png_path):\n",
    "    if png_path != \"\": \n",
    "        encoded_image = encode_png_file(png_path)\n",
    "    \n",
    "        response = ollama.chat(\n",
    "           model=model,\n",
    "           messages=[{'role': 'user', 'content': prompt,'images': [encoded_image]}],\n",
    "           options={\n",
    "               'temperature': 0.7,  # Range: 0.0 to 1.0\n",
    "               'keep_alive': 30\n",
    "           }\n",
    "        )\n",
    "    else:\n",
    "        response = ollama.chat(\n",
    "           model=model,\n",
    "           messages=[{'role': 'user', 'content': prompt}],\n",
    "           options={\n",
    "                'temperature': 0.7,  # Range: 0.0 to 1.0\n",
    "                'keep_alive': 30\n",
    "           }\n",
    "        )\n",
    "\n",
    "    foo = response.message.content[8:-4]\n",
    "    #print(response.message.content)\n",
    "    response_J = json.loads(foo)\n",
    "    return response_J\n",
    "\n",
    "def query_ollama_multiple(model, prompt, png_path): #same as above, except png_path is a list of pngs\n",
    "    #!!!!!This does not work. Right now ollama does not support multiple images\n",
    "    encoded_images = []\n",
    "    \n",
    "    for i,png in enumerate(png_path):\n",
    "       encoded_images.append(encode_png_file(png))\n",
    "   \n",
    "    response = ollama.chat(\n",
    "       model=model,\n",
    "       messages=[{'role': 'user', 'content': prompt,'images': [encoded_images]}],\n",
    "       options={\n",
    "           'temperature': 0.7,  # Range: 0.0 to 1.0\n",
    "       }\n",
    "    )\n",
    "\n",
    "    foo = response.message.content[8:-4]\n",
    "    response_J = json.loads(foo)\n",
    "    return response_J    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "034780ff-bc32-417f-9c1e-35a47ea5a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Claude\n",
    "Claude = \"claude-sonnet-4-20250514\"\n",
    "\n",
    "#initialize\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=key  # or set ANTHROPIC_API_KEY environment variable\n",
    ")\n",
    "\n",
    "def claude_simple(prompt_text):\n",
    "    content = []\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": prompt_text\n",
    "    })\n",
    "    \n",
    "    message = client.messages.create(\n",
    "        model=Claude,\n",
    "        max_tokens=2000,\n",
    "        temperature = 0.5,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content\n",
    "        }]\n",
    "    )\n",
    "    return message.content[0].text \n",
    "\n",
    "def upload_multiple_images_to_claude(image_paths, prompt_text):\n",
    "    \"\"\"\n",
    "    Upload multiple images to Claude\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): List of paths to image files\n",
    "        prompt_text (str): Text prompt to send along with the images\n",
    "    \n",
    "    Returns:\n",
    "        str: Claude's response\n",
    "    \"\"\"\n",
    "    \n",
    "    content = []\n",
    "    \n",
    "    # Add all images\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        image_path = Path(image_path)\n",
    "        \n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "        \n",
    "        content.append({\n",
    "            \"type\": \"image\",\n",
    "            \"source\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": \"image/png\",\n",
    "                \"data\": image_data\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Add text prompt\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": prompt_text\n",
    "    })\n",
    "    \n",
    "    message = client.messages.create(\n",
    "        model=Claude,\n",
    "        max_tokens=2000,\n",
    "        temperature = 0.5,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6daefdd6-baa8-47cc-9510-df6afd5d7749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping YOLO predictions\n",
    "def crop_predictions(png_file_path, predictions_csv_path, scaling_factor=1.0, output_dir=\"cropped_images\"):\n",
    "    \"\"\"\n",
    "    Crop image regions based on predictions CSV with bounding box coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    png_file_path (str): Path to the input PNG image\n",
    "    predictions_csv_path (str): Path to the predictions CSV file\n",
    "    scaling_factor (float): Factor to scale the bounding box size (1.0 = original size)\n",
    "    output_dir (str): Directory to save cropped images\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(predictions_csv_path)\n",
    "    \n",
    "    # Load the image\n",
    "    image = Image.open(png_file_path)\n",
    "    image_width, image_height = image.size\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get the base name of the PNG file (without extension)\n",
    "    base_name = Path(png_file_path).stem\n",
    "    \n",
    "    cropped_images = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Parse the xyxy coordinates from string representation\n",
    "        # Remove 'tensor()' wrapper and convert to float\n",
    "        coords_str = row['Xyxy'].strip('[]').replace(\" device='cuda:0'),\",\"\")\n",
    "        coords_parts = coords_str.split(', ')\n",
    "        if len(coords_parts)>4: # there is extra garbage on the end, remove it\n",
    "            del coords_parts[-1]\n",
    "        \n",
    "        # Extract numerical values from tensor format\n",
    "        coords = []\n",
    "        for part in coords_parts:\n",
    "            # Remove 'tensor(' and ')' and convert to float\n",
    "            num_str = part.strip().replace('tensor(', '').replace(')', '').replace('.', '')\n",
    "            coords.append(float(num_str))\n",
    "        \n",
    "        x1, y1, x2, y2 = coords\n",
    "        \n",
    "        # Calculate center and dimensions of bounding box\n",
    "        center_x = (x1 + x2) / 2\n",
    "        center_y = (y1 + y2) / 2\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        \n",
    "        # Apply scaling factor\n",
    "        scaled_width = width * scaling_factor\n",
    "        scaled_height = height * scaling_factor\n",
    "        \n",
    "        # Calculate new coordinates\n",
    "        new_x1 = center_x - scaled_width / 2\n",
    "        new_y1 = center_y - scaled_height / 2\n",
    "        new_x2 = center_x + scaled_width / 2\n",
    "        new_y2 = center_y + scaled_height / 2\n",
    "        \n",
    "        # Ensure coordinates are within image bounds\n",
    "        new_x1 = max(0, min(new_x1, image_width))\n",
    "        new_y1 = max(0, min(new_y1, image_height))\n",
    "        new_x2 = max(0, min(new_x2, image_width))\n",
    "        new_y2 = max(0, min(new_y2, image_height))\n",
    "        \n",
    "        # Crop the image\n",
    "        cropped_img = image.crop((new_x1, new_y1, new_x2, new_y2))\n",
    "        \n",
    "        # Create filename for cropped image\n",
    "        prediction_clean = row['Prediction']\n",
    "        confidence = f\"{row['Confidence']:.2f}\"\n",
    "        output_filename = f\"{base_name}_{idx:03d}_{prediction_clean}_conf{confidence}.png\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "        # Save cropped image\n",
    "        cropped_img.save(output_path)\n",
    "        \n",
    "        cropped_images.append({\n",
    "            'index': idx,\n",
    "            'prediction': row['Prediction'],\n",
    "            'confidence': row['Confidence'],\n",
    "            'original_coords': (x1, y1, x2, y2),\n",
    "            'scaled_coords': (new_x1, new_y1, new_x2, new_y2),\n",
    "            'output_path': output_path,\n",
    "            'size': cropped_img.size\n",
    "        })\n",
    "        \n",
    "        print(f\"Saved: {output_filename} ({cropped_img.size[0]}x{cropped_img.size[1]})\")\n",
    "    \n",
    "    print(f\"\\nProcessed {len(cropped_images)} predictions\")\n",
    "    print(f\"Cropped images saved to: {output_dir}\")\n",
    "    \n",
    "    return cropped_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aa6f80f-df86-4887-b5ab-c75aff351dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "valve_classes = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "inst_classes = [26,27,28,29,31]\n",
    "def class_lookup(prediction):\n",
    "    res = \"unknown\" # if not a match\n",
    "    match prediction:\n",
    "        case 1:\n",
    "            res = \"Gate_Valve\"\n",
    "        case 2:\n",
    "            res = \"Ball_Valve\"\n",
    "        case 3:\n",
    "            res = \"Globe_valve_NO\"\n",
    "        case 4: \n",
    "            res = \"Gate_valve_NO\"\n",
    "        case 5: \n",
    "            res = \"Globe_valve_NO\"\n",
    "        case 6: \n",
    "            res = \"Butterfly_Valve\"\n",
    "        case 7: \n",
    "            res = \"Plug Valve\"\n",
    "        case 8: \n",
    "            res = \"Check_Valve\"\n",
    "        case 9: \n",
    "            res = \"Diaphragm_valve\"\n",
    "        case 10: \n",
    "            res = \"Needle_valve\"\n",
    "        case 11: \n",
    "            res = \"Half_Filled_Gate_Valve\"\n",
    "        case 12: \n",
    "            res = \"Gate_Valve_NC\"\n",
    "        case 13: \n",
    "            res = \"Globle_valve_NC\"\n",
    "        case 14: \n",
    "            res = \"Control_Valve\"\n",
    "        case 15:\n",
    "            res = \"Rotary_Valve\"\n",
    "        case 16: \n",
    "            res = \"Ball_valve_NC\"\n",
    "        case 17:\n",
    "            res = \"Paddle_blind\"\n",
    "        case 18: \n",
    "            res = \"Spectacle_blind_Closed\"\n",
    "        case 19: \n",
    "            res = \"Spectacle_blind_Open\"\n",
    "        case 20: \n",
    "            res = \"Reducer\"\n",
    "        case 21:\n",
    "            res = \"Flange_or_Nozzle\"\n",
    "        case 22: \n",
    "            res = \"Rupture_disk\"\n",
    "        case 23: \n",
    "            res = \"Pipe_Insulation_or_Tracing\"\n",
    "        case 24:\n",
    "            res = \"Flow_Arrow\"\n",
    "        case 25:\n",
    "            res = \"sight_glass\"\n",
    "        case 26:\n",
    "            res = \"Instrument_Field\"\n",
    "        case 27: \n",
    "            res = \"Instrument_Field\"\n",
    "        case 28: \n",
    "            res = \"Instrument_Panel\"\n",
    "        case 29:\n",
    "            res = \"Instrument_Aux_Panel\"\n",
    "        case 30:\n",
    "            res = \"box\"\n",
    "        case 31:\n",
    "            res = \"Instrument_Panel\"\n",
    "        case 32:\n",
    "            res = \"box\"\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a2ac6c1-d49c-4493-923a-2e8580605b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_valves_instruments(flavor,resultslist,full_dwg,detect_path):\n",
    "# flavor = \"valve\" or \"inst\"\n",
    "# resultlist is the list of valves or instruments to return\n",
    "# classlist is the list of valve or instrument predictions valve_classlist or inst_classlist, depending on the \"flavor\"\n",
    "# scale will also be defined as 1.1 for inst and 8.0 for valves\n",
    "    if flavor == \"valve\":\n",
    "        scale = 8\n",
    "        classlist = valve_classes\n",
    "    elif flavor == \"inst\":\n",
    "        scale = 1.1\n",
    "        classlist = inst_classes\n",
    "    else:\n",
    "        print (\"error, undefined flavor, must be inst or valve\")\n",
    "        return\n",
    "    for quad in range(4):\n",
    "        #image size is important - if the size is too far off, detection is not accurate\n",
    "        #using cpu because otherwise out of memory issues intermittently. Speed difference is not material\n",
    "        run_path = '\"./detect.py\" --weights \"./best.pt\" --source '+full_dwg[quad]+' --conf-thres 0.5 --save-csv --imgsz 5000 --device cpu'\n",
    "        %run {run_path}\n",
    "        result_path = detect_path+\"/exp\"\n",
    "        if quad>0:\n",
    "            result_path = result_path+str(quad+1)\n",
    "        #create cropped images. scale needs to be big for valves, small for instruments\n",
    "        crop_predictions(full_dwg[quad], result_path+\"/predictions.csv\", scaling_factor=scale, output_dir=result_path)\n",
    "        #go through each detected item and add the ones we want to the valve list\n",
    "            # Read the CSV file\n",
    "        df = pd.read_csv(result_path+\"/predictions.csv\")\n",
    "        base_name = Path(full_dwg[quad]).stem\n",
    "        for idx, row in df.iterrows():\n",
    "            # Parse the xyxy coordinates from string representation\n",
    "            # Remove 'tensor()' wrapper and convert to float\n",
    "            coords_str = row['Xyxy'].strip('[]').replace(\" device='cuda:0'),\",\"\")\n",
    "            coords_parts = coords_str.split(', ')\n",
    "            if len(coords_parts)>4: # there is extra garbage on the end, remove it\n",
    "                del coords_parts[-1]\n",
    "            prediction = row['Prediction']\n",
    "            #Prediction holds an int containing the type of item found. We only want certain types\n",
    "            if prediction in classlist: # this is a valve (or this is an instrument)\n",
    "                classname = class_lookup(prediction)\n",
    "                # Extract numerical values from tensor format to get the valve center coordinates\n",
    "                coords = []\n",
    "                for part in coords_parts:\n",
    "                # Remove 'tensor(' and ')' and convert to float\n",
    "                    num_str = part.strip().replace('tensor(', '').replace(')', '').replace('.', '')\n",
    "                    coords.append(float(num_str))\n",
    "                x1, y1, x2, y2 = coords\n",
    "                # Calculate center and dimensions of bounding box\n",
    "                center_x = (x1 + x2) / 2\n",
    "                center_y = (y1 + y2) / 2\n",
    "                #Get the image filename and send it to a LLM to get the size and tag number\n",
    "                # reCreate filename for cropped image\n",
    "                prediction_clean = row['Prediction']\n",
    "                confidence = f\"{row['Confidence']:.2f}\"\n",
    "                output_filename = f\"{base_name}_{idx:03d}_{prediction_clean}_conf{confidence}.png\"\n",
    "                png_path = os.path.join(result_path, output_filename)\n",
    "                if flavor == \"valve\":\n",
    "                    prompt = \"\"\"Look at the valve in the center of the image. Return the valve size and valve tag number as a \n",
    "                    json string. If either value can't be found return the value as none\"\"\"\n",
    "                    result = query_ollama(ollama_model, prompt, png_path)\n",
    "                    valve_size=\"\"\n",
    "                    valve_tag=\"\"\n",
    "                    try:\n",
    "                       valve_size = result[\"valve_size\"]\n",
    "                       valve_tag = result[\"valve_tag_number\"]\n",
    "                    except KeyError as e:\n",
    "                        valve_tag = result[\"valve_tag\"]\n",
    "                    except:\n",
    "                        print(\"error:\",result)\n",
    "#FUTURE: try to get the line number of the valve too\n",
    "                    valvedict = {\"tag\": valve_tag, \"type\": classname, \"size\": valve_size, \"center_x\": center_x, \"center_y\": center_y}\n",
    "                    resultslist.append(valvedict)\n",
    "                else:\n",
    "                    prompt = \"\"\"Look at the instrument in the center of the image. The instrument tag type is the top text\n",
    "                    and the instrument number is the bottom text. Return a JSON string of top and bottom text with the keys top and bottom\"\"\"\n",
    "                    result = query_ollama(ollama_model, prompt, png_path)\n",
    "                    print(result)\n",
    "                    top=\"\"\n",
    "                    bottom=\"\"\n",
    "                    try:\n",
    "                        top = result[\"top\"]\n",
    "                        bottom = result[\"bottom\"]\n",
    "                        inst_tag = str(top) + \"-\" + str(bottom)\n",
    "                    except KeyError as e:\n",
    "                        inst_tag = top+bottom\n",
    "                    except:\n",
    "                        print(\"error:\",result)\n",
    "\n",
    "                    instdict = {\"tag\": inst_tag, \"type\": classname, \"center_x\": center_x, \"center_y\": center_y}\n",
    "                    resultslist.append(instdict)\n",
    "    return resultslist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fce5e59e-8b38-4230-b326-b86306b87986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: detect.py [-h] [--weights WEIGHTS [WEIGHTS ...]] [--source SOURCE]\n",
      "                 [--data DATA] [--imgsz IMGSZ [IMGSZ ...]]\n",
      "                 [--conf-thres CONF_THRES] [--iou-thres IOU_THRES]\n",
      "                 [--max-det MAX_DET] [--device DEVICE] [--view-img]\n",
      "                 [--save-txt] [--save-format SAVE_FORMAT] [--save-csv]\n",
      "                 [--save-conf] [--save-crop] [--nosave]\n",
      "                 [--classes CLASSES [CLASSES ...]] [--agnostic-nms]\n",
      "                 [--augment] [--visualize] [--update] [--project PROJECT]\n",
      "                 [--name NAME] [--exist-ok] [--line-thickness LINE_THICKNESS]\n",
      "                 [--hide-labels] [--hide-conf] [--half] [--dnn]\n",
      "                 [--vid-stride VID_STRIDE]\n",
      "detect.py: error: unrecognized arguments: true\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/peter/Desktop/yolov5/runs/detect/exp/predictions.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m    \u001b[38;5;66;03m#don't sweat it if the directory doesn't exist (anymore)\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# go through each prediction and add the right info to the valve list\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m#process_valves(valves,full_dwg,detect_path)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43mprocess_valves_instruments\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalve\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvalves\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfull_dwg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdetect_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m#Instruments\u001b[39;00m\n\u001b[32m     48\u001b[39m instruments = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mprocess_valves_instruments\u001b[39m\u001b[34m(flavor, resultslist, full_dwg, detect_path)\u001b[39m\n\u001b[32m     22\u001b[39m     result_path = result_path+\u001b[38;5;28mstr\u001b[39m(quad+\u001b[32m1\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m#create cropped images. scale needs to be big for valves, small for instruments\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mcrop_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_dwg\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquad\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_path\u001b[49m\u001b[43m+\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/predictions.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresult_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m#go through each detected item and add the ones we want to the valve list\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# Read the CSV file\u001b[39;00m\n\u001b[32m     27\u001b[39m df = pd.read_csv(result_path+\u001b[33m\"\u001b[39m\u001b[33m/predictions.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mcrop_predictions\u001b[39m\u001b[34m(png_file_path, predictions_csv_path, scaling_factor, output_dir)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mCrop image regions based on predictions CSV with bounding box coordinates.\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[33;03moutput_dir (str): Directory to save cropped images\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Read the CSV file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions_csv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Load the image\u001b[39;00m\n\u001b[32m     17\u001b[39m image = Image.open(png_file_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/pipx/venvs/notebook/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/pipx/venvs/notebook/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/pipx/venvs/notebook/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/pipx/venvs/notebook/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/pipx/venvs/notebook/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/peter/Desktop/yolov5/runs/detect/exp/predictions.csv'"
     ]
    }
   ],
   "source": [
    "#main\n",
    "#pdf_file = \"/home/peter/ML101620329.pdf\"\n",
    "pdf_file = \"/home/peter/PD637-A-2010-1-Model.pdf\"\n",
    "page = 0\n",
    "\n",
    "ollama_model = 'gemma3:27b'\n",
    "\n",
    "output_dir = pdf_file[:-4]\n",
    "filename = os.path.splitext(os.path.basename(pdf_file))[0]\n",
    "create_quads(pdf_file,page,output_dir) #create PNG files\n",
    "\n",
    "# Call Gemma3:27b via Ollama to get metadata\n",
    "prompt = \"\"\"List the project name, company name, drawing (DWG) name, drawing number, location, date, revision, author, \n",
    "and any other information found in the title block in the lower right corner of the image. Return data as JSON.\"\"\"\n",
    "png_path = output_dir+\"/\"+filename+\"_q4.png\"\n",
    "drawing_info = query_ollama(ollama_model, prompt, png_path\n",
    "                           )\n",
    "# Call Claude and get line list. Need to send all 4 quadrants\n",
    "full_dwg = [] #all 4 quadrants\n",
    "for i in range(4):\n",
    "    n = i+1\n",
    "    full_dwg.append(output_dir+\"/\"+filename+\"_q\"+str(n)+\".png\")\n",
    "prompt = \"\"\"list the pipe lines on this P&ID with the line number, primary size, and what equipment, off-page connectors, \n",
    "or other lines it is connected to. Return the data as JSON and only return JSON.\"\"\"\n",
    "result = upload_multiple_images_to_claude(full_dwg, prompt)\n",
    "#TODO: there is a limit to the length of result, probably due to token limit. If there are more than 30 lines on the dwg, it will fail. \n",
    "line_list = json.loads(result[8:-4])\n",
    "\n",
    "#equipment list\n",
    "prompt = \"\"\"List the equipment on this P&ID with the name, description and associated metadata. Only list the primary equipment. \n",
    "Do not list piping, valves, instruments or information about the drawing itself. Return data as JSON\"\"\"\n",
    "result = upload_multiple_images_to_claude(full_dwg, prompt)\n",
    "equip_list = json.loads(result[8:-4])\n",
    "\n",
    "#Valves\n",
    "valves = []\n",
    "#clean up previous runs\n",
    "try:\n",
    "    detect_path = \"/home/peter/Desktop/yolov5/runs/detect\"\n",
    "    shutil.rmtree(detect_path)\n",
    "except FileNotFoundError :\n",
    "    pass    #don't sweat it if the directory doesn't exist (anymore)\n",
    "\n",
    "# go through each prediction and add the right info to the valve list\n",
    "#process_valves(valves,full_dwg,detect_path)\n",
    "process_valves_instruments(\"valve\",valves,full_dwg,detect_path)\n",
    "#Instruments\n",
    "instruments = []\n",
    "process_valves_instruments(\"inst\",instruments,full_dwg,detect_path)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ca1f864-83c4-4dc9-9895-2f6d17e8a17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['./best.pt'], source=/home/peter/PD637-A-2010-1-Model/PD637-A-2010-1-Model_q1.png, data=data/coco128.yaml, imgsz=[5000, 5000], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=cpu, view_img=False, save_txt=False, save_format=0, save_csv=True, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['gitpython>=3.1.30'] not found, attempting AutoUpdate...\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 0.0s\n",
      "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 v7.0-430-g459d8bf0 Python-3.12.3 torch-2.8.0+cu128 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 267 layers, 46280598 parameters, 0 gradients, 108.2 GFLOPs\n",
      "WARNING ⚠️ --img-size [5000, 5000] must be multiple of max stride 32, updating to [5024, 5024]\n",
      "image 1/1 /home/peter/PD637-A-2010-1-Model/PD637-A-2010-1-Model_q1.png: 3552x5024 7 4s, 9 12s, 3 13s, 2 19s, 1 20, 1 21, 9 23s, 9 24s, 3 26s, 2 32s, 8879.1ms\n",
      "Speed: 13.3ms pre-process, 8879.1ms inference, 20.6ms NMS per image at shape (1, 3, 5024, 5024)\n",
      "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: PD637-A-2010-1-Model_q1_000_32_conf0.56.png (128x59)\n",
      "Saved: PD637-A-2010-1-Model_q1_001_24_conf0.63.png (85x42)\n",
      "Saved: PD637-A-2010-1-Model_q1_002_13_conf0.63.png (23x45)\n",
      "Saved: PD637-A-2010-1-Model_q1_003_24_conf0.64.png (45x67)\n",
      "Saved: PD637-A-2010-1-Model_q1_004_4_conf0.64.png (91x49)\n",
      "Saved: PD637-A-2010-1-Model_q1_005_26_conf0.65.png (38x44)\n",
      "Saved: PD637-A-2010-1-Model_q1_006_26_conf0.66.png (173x179)\n",
      "Saved: PD637-A-2010-1-Model_q1_007_21_conf0.67.png (31x18)\n",
      "Saved: PD637-A-2010-1-Model_q1_008_23_conf0.68.png (129x56)\n",
      "Saved: PD637-A-2010-1-Model_q1_009_26_conf0.68.png (172x178)\n",
      "Saved: PD637-A-2010-1-Model_q1_010_24_conf0.69.png (81x46)\n",
      "Saved: PD637-A-2010-1-Model_q1_011_24_conf0.72.png (44x74)\n",
      "Saved: PD637-A-2010-1-Model_q1_012_24_conf0.72.png (73x66)\n",
      "Saved: PD637-A-2010-1-Model_q1_013_24_conf0.72.png (73x67)\n",
      "Saved: PD637-A-2010-1-Model_q1_014_13_conf0.73.png (23x46)\n",
      "Saved: PD637-A-2010-1-Model_q1_015_24_conf0.75.png (44x71)\n",
      "Saved: PD637-A-2010-1-Model_q1_016_32_conf0.75.png (131x58)\n",
      "Saved: PD637-A-2010-1-Model_q1_017_4_conf0.75.png (89x48)\n",
      "Saved: PD637-A-2010-1-Model_q1_018_23_conf0.77.png (129x54)\n",
      "Saved: PD637-A-2010-1-Model_q1_019_20_conf0.82.png (50x86)\n",
      "Saved: PD637-A-2010-1-Model_q1_020_12_conf0.82.png (46x86)\n",
      "Saved: PD637-A-2010-1-Model_q1_021_24_conf0.82.png (79x42)\n",
      "Saved: PD637-A-2010-1-Model_q1_022_23_conf0.84.png (133x57)\n",
      "Saved: PD637-A-2010-1-Model_q1_023_4_conf0.84.png (90x47)\n",
      "Saved: PD637-A-2010-1-Model_q1_024_4_conf0.85.png (88x49)\n",
      "Saved: PD637-A-2010-1-Model_q1_025_24_conf0.85.png (74x44)\n",
      "Saved: PD637-A-2010-1-Model_q1_026_23_conf0.86.png (129x57)\n",
      "Saved: PD637-A-2010-1-Model_q1_027_4_conf0.86.png (88x49)\n",
      "Saved: PD637-A-2010-1-Model_q1_028_13_conf0.86.png (23x45)\n",
      "Saved: PD637-A-2010-1-Model_q1_029_23_conf0.86.png (130x58)\n",
      "Saved: PD637-A-2010-1-Model_q1_030_23_conf0.86.png (130x58)\n",
      "Saved: PD637-A-2010-1-Model_q1_031_23_conf0.87.png (126x57)\n",
      "Saved: PD637-A-2010-1-Model_q1_032_23_conf0.87.png (128x58)\n",
      "Saved: PD637-A-2010-1-Model_q1_033_23_conf0.88.png (129x57)\n",
      "Saved: PD637-A-2010-1-Model_q1_034_19_conf0.89.png (127x48)\n",
      "Saved: PD637-A-2010-1-Model_q1_035_4_conf0.89.png (88x48)\n",
      "Saved: PD637-A-2010-1-Model_q1_036_12_conf0.89.png (87x45)\n",
      "Saved: PD637-A-2010-1-Model_q1_037_12_conf0.90.png (87x44)\n",
      "Saved: PD637-A-2010-1-Model_q1_038_12_conf0.90.png (88x44)\n",
      "Saved: PD637-A-2010-1-Model_q1_039_12_conf0.90.png (49x89)\n",
      "Saved: PD637-A-2010-1-Model_q1_040_19_conf0.91.png (49x131)\n",
      "Saved: PD637-A-2010-1-Model_q1_041_4_conf0.94.png (50x89)\n",
      "Saved: PD637-A-2010-1-Model_q1_042_12_conf0.94.png (47x85)\n",
      "Saved: PD637-A-2010-1-Model_q1_043_12_conf0.94.png (46x87)\n",
      "Saved: PD637-A-2010-1-Model_q1_044_12_conf0.95.png (46x85)\n",
      "Saved: PD637-A-2010-1-Model_q1_045_12_conf0.95.png (46x86)\n",
      "\n",
      "Processed 46 predictions\n",
      "Cropped images saved to: /home/peter/Desktop/yolov5/runs/detect/exp\n",
      "{'top': '@', 'bottom': 'a'}\n",
      "{'top': 'PG', 'bottom': '20051'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['./best.pt'], source=/home/peter/PD637-A-2010-1-Model/PD637-A-2010-1-Model_q2.png, data=data/coco128.yaml, imgsz=[5000, 5000], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=cpu, view_img=False, save_txt=False, save_format=0, save_csv=True, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'top': 'LG', 'bottom': '20001'}\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['gitpython>=3.1.30'] not found, attempting AutoUpdate...\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 0.0s\n",
      "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 v7.0-430-g459d8bf0 Python-3.12.3 torch-2.8.0+cu128 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 267 layers, 46280598 parameters, 0 gradients, 108.2 GFLOPs\n",
      "WARNING ⚠️ --img-size [5000, 5000] must be multiple of max stride 32, updating to [5024, 5024]\n",
      "image 1/1 /home/peter/PD637-A-2010-1-Model/PD637-A-2010-1-Model_q2.png: 3552x5024 1 4, 1 12, 3 23s, 2 24s, 10038.8ms\n",
      "Speed: 13.4ms pre-process, 10038.8ms inference, 19.0ms NMS per image at shape (1, 3, 5024, 5024)\n",
      "Results saved to \u001b[1mruns/detect/exp2\u001b[0m\n",
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['./best.pt'], source=/home/peter/PD637-A-2010-1-Model/PD637-A-2010-1-Model_q3.png, data=data/coco128.yaml, imgsz=[5000, 5000], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=cpu, view_img=False, save_txt=False, save_format=0, save_csv=True, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: PD637-A-2010-1-Model_q2_000_23_conf0.59.png (131x58)\n",
      "Saved: PD637-A-2010-1-Model_q2_001_24_conf0.68.png (43x71)\n",
      "Saved: PD637-A-2010-1-Model_q2_002_24_conf0.75.png (44x71)\n",
      "Saved: PD637-A-2010-1-Model_q2_003_23_conf0.80.png (58x134)\n",
      "Saved: PD637-A-2010-1-Model_q2_004_12_conf0.90.png (89x44)\n",
      "Saved: PD637-A-2010-1-Model_q2_005_23_conf0.90.png (128x58)\n",
      "Saved: PD637-A-2010-1-Model_q2_006_4_conf0.94.png (49x89)\n",
      "\n",
      "Processed 7 predictions\n",
      "Cropped images saved to: /home/peter/Desktop/yolov5/runs/detect/exp2\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['gitpython>=3.1.30'] not found, attempting AutoUpdate...\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 0.0s\n",
      "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 v7.0-430-g459d8bf0 Python-3.12.3 torch-2.8.0+cu128 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 267 layers, 46280598 parameters, 0 gradients, 108.2 GFLOPs\n",
      "WARNING ⚠️ --img-size [5000, 5000] must be multiple of max stride 32, updating to [5024, 5024]\n",
      "image 1/1 /home/peter/PD637-A-2010-1-Model/PD637-A-2010-1-Model_q3.png: 3552x5024 3 4s, 5 12s, 3 19s, 2 20s, 1 23, 4 24s, 10032.2ms\n",
      "Speed: 13.4ms pre-process, 10032.2ms inference, 16.0ms NMS per image at shape (1, 3, 5024, 5024)\n",
      "Results saved to \u001b[1mruns/detect/exp3\u001b[0m\n",
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['./best.pt'], source=/home/peter/PD637-A-2010-1-Model/PD637-A-2010-1-Model_q4.png, data=data/coco128.yaml, imgsz=[5000, 5000], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=cpu, view_img=False, save_txt=False, save_format=0, save_csv=True, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: PD637-A-2010-1-Model_q3_000_24_conf0.65.png (47x85)\n",
      "Saved: PD637-A-2010-1-Model_q3_001_24_conf0.66.png (25x48)\n",
      "Saved: PD637-A-2010-1-Model_q3_002_20_conf0.73.png (50x85)\n",
      "Saved: PD637-A-2010-1-Model_q3_003_24_conf0.77.png (26x57)\n",
      "Saved: PD637-A-2010-1-Model_q3_004_20_conf0.77.png (51x84)\n",
      "Saved: PD637-A-2010-1-Model_q3_005_19_conf0.82.png (49x131)\n",
      "Saved: PD637-A-2010-1-Model_q3_006_19_conf0.83.png (49x133)\n",
      "Saved: PD637-A-2010-1-Model_q3_007_24_conf0.84.png (74x42)\n",
      "Saved: PD637-A-2010-1-Model_q3_008_19_conf0.84.png (130x49)\n",
      "Saved: PD637-A-2010-1-Model_q3_009_23_conf0.87.png (59x133)\n",
      "Saved: PD637-A-2010-1-Model_q3_010_4_conf0.88.png (91x49)\n",
      "Saved: PD637-A-2010-1-Model_q3_011_4_conf0.89.png (90x48)\n",
      "Saved: PD637-A-2010-1-Model_q3_012_4_conf0.90.png (88x47)\n",
      "Saved: PD637-A-2010-1-Model_q3_013_12_conf0.94.png (47x87)\n",
      "Saved: PD637-A-2010-1-Model_q3_014_12_conf0.95.png (46x86)\n",
      "Saved: PD637-A-2010-1-Model_q3_015_12_conf0.95.png (46x86)\n",
      "Saved: PD637-A-2010-1-Model_q3_016_12_conf0.96.png (46x87)\n",
      "Saved: PD637-A-2010-1-Model_q3_017_12_conf0.96.png (46x88)\n",
      "\n",
      "Processed 18 predictions\n",
      "Cropped images saved to: /home/peter/Desktop/yolov5/runs/detect/exp3\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['gitpython>=3.1.30'] not found, attempting AutoUpdate...\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 0.0s\n",
      "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 v7.0-430-g459d8bf0 Python-3.12.3 torch-2.8.0+cu128 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 267 layers, 46280598 parameters, 0 gradients, 108.2 GFLOPs\n",
      "WARNING ⚠️ --img-size [5000, 5000] must be multiple of max stride 32, updating to [5024, 5024]\n",
      "image 1/1 /home/peter/PD637-A-2010-1-Model/PD637-A-2010-1-Model_q4.png: 3552x5024 1 3, 9 4s, 2 6s, 12 12s, 6 19s, 3 20s, 6 21s, 1 22, 1 23, 16 24s, 4 26s, 2 30s, 4 31s, 1 32, 10446.6ms\n",
      "Speed: 14.4ms pre-process, 10446.6ms inference, 21.1ms NMS per image at shape (1, 3, 5024, 5024)\n",
      "Results saved to \u001b[1mruns/detect/exp4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: PD637-A-2010-1-Model_q4_000_19_conf0.56.png (131x44)\n",
      "Saved: PD637-A-2010-1-Model_q4_001_22_conf0.57.png (58x133)\n",
      "Saved: PD637-A-2010-1-Model_q4_002_3_conf0.58.png (54x90)\n",
      "Saved: PD637-A-2010-1-Model_q4_003_26_conf0.61.png (172x180)\n",
      "Saved: PD637-A-2010-1-Model_q4_004_24_conf0.64.png (71x44)\n",
      "Saved: PD637-A-2010-1-Model_q4_005_26_conf0.64.png (173x176)\n",
      "Saved: PD637-A-2010-1-Model_q4_006_32_conf0.64.png (161x90)\n",
      "Saved: PD637-A-2010-1-Model_q4_007_24_conf0.66.png (42x68)\n",
      "Saved: PD637-A-2010-1-Model_q4_008_31_conf0.66.png (169x162)\n",
      "Saved: PD637-A-2010-1-Model_q4_009_26_conf0.66.png (171x159)\n",
      "Saved: PD637-A-2010-1-Model_q4_010_24_conf0.67.png (43x69)\n",
      "Saved: PD637-A-2010-1-Model_q4_011_26_conf0.67.png (173x182)\n",
      "Saved: PD637-A-2010-1-Model_q4_012_31_conf0.69.png (168x164)\n",
      "Saved: PD637-A-2010-1-Model_q4_013_24_conf0.72.png (54x31)\n",
      "Saved: PD637-A-2010-1-Model_q4_014_4_conf0.72.png (50x87)\n",
      "Saved: PD637-A-2010-1-Model_q4_015_24_conf0.73.png (78x44)\n",
      "Saved: PD637-A-2010-1-Model_q4_016_19_conf0.74.png (48x135)\n",
      "Saved: PD637-A-2010-1-Model_q4_017_24_conf0.74.png (28x54)\n",
      "Saved: PD637-A-2010-1-Model_q4_018_24_conf0.76.png (30x59)\n",
      "Saved: PD637-A-2010-1-Model_q4_019_21_conf0.77.png (28x16)\n",
      "Saved: PD637-A-2010-1-Model_q4_020_30_conf0.79.png (128x132)\n",
      "Saved: PD637-A-2010-1-Model_q4_021_24_conf0.80.png (34x63)\n",
      "Saved: PD637-A-2010-1-Model_q4_022_21_conf0.81.png (27x15)\n",
      "Saved: PD637-A-2010-1-Model_q4_023_19_conf0.81.png (45x131)\n",
      "Saved: PD637-A-2010-1-Model_q4_024_30_conf0.81.png (129x132)\n",
      "Saved: PD637-A-2010-1-Model_q4_025_23_conf0.82.png (58x135)\n",
      "Saved: PD637-A-2010-1-Model_q4_026_20_conf0.82.png (87x49)\n",
      "Saved: PD637-A-2010-1-Model_q4_027_31_conf0.82.png (171x167)\n",
      "Saved: PD637-A-2010-1-Model_q4_028_24_conf0.82.png (57x30)\n",
      "Saved: PD637-A-2010-1-Model_q4_029_19_conf0.83.png (131x46)\n",
      "Saved: PD637-A-2010-1-Model_q4_030_6_conf0.83.png (88x48)\n",
      "Saved: PD637-A-2010-1-Model_q4_031_6_conf0.83.png (89x48)\n",
      "Saved: PD637-A-2010-1-Model_q4_032_24_conf0.85.png (57x30)\n",
      "Saved: PD637-A-2010-1-Model_q4_033_24_conf0.85.png (56x31)\n",
      "Saved: PD637-A-2010-1-Model_q4_034_21_conf0.85.png (27x15)\n",
      "Saved: PD637-A-2010-1-Model_q4_035_24_conf0.85.png (59x30)\n",
      "Saved: PD637-A-2010-1-Model_q4_036_19_conf0.85.png (46x132)\n",
      "Saved: PD637-A-2010-1-Model_q4_037_20_conf0.85.png (86x50)\n",
      "Saved: PD637-A-2010-1-Model_q4_038_31_conf0.86.png (170x164)\n",
      "Saved: PD637-A-2010-1-Model_q4_039_4_conf0.87.png (89x47)\n",
      "Saved: PD637-A-2010-1-Model_q4_040_24_conf0.87.png (30x59)\n",
      "Saved: PD637-A-2010-1-Model_q4_041_24_conf0.87.png (53x30)\n",
      "Saved: PD637-A-2010-1-Model_q4_042_21_conf0.88.png (60x30)\n",
      "Saved: PD637-A-2010-1-Model_q4_043_24_conf0.88.png (30x59)\n",
      "Saved: PD637-A-2010-1-Model_q4_044_20_conf0.88.png (86x50)\n",
      "Saved: PD637-A-2010-1-Model_q4_045_24_conf0.89.png (70x43)\n",
      "Saved: PD637-A-2010-1-Model_q4_046_21_conf0.89.png (60x30)\n",
      "Saved: PD637-A-2010-1-Model_q4_047_4_conf0.89.png (88x47)\n",
      "Saved: PD637-A-2010-1-Model_q4_048_19_conf0.89.png (48x131)\n",
      "Saved: PD637-A-2010-1-Model_q4_049_12_conf0.89.png (87x43)\n",
      "Saved: PD637-A-2010-1-Model_q4_050_4_conf0.90.png (89x48)\n",
      "Saved: PD637-A-2010-1-Model_q4_051_21_conf0.90.png (59x30)\n",
      "Saved: PD637-A-2010-1-Model_q4_052_12_conf0.90.png (87x44)\n",
      "Saved: PD637-A-2010-1-Model_q4_053_4_conf0.91.png (48x88)\n",
      "Saved: PD637-A-2010-1-Model_q4_054_4_conf0.91.png (49x88)\n",
      "Saved: PD637-A-2010-1-Model_q4_055_12_conf0.92.png (88x45)\n",
      "Saved: PD637-A-2010-1-Model_q4_056_12_conf0.93.png (47x86)\n",
      "Saved: PD637-A-2010-1-Model_q4_057_4_conf0.93.png (47x88)\n",
      "Saved: PD637-A-2010-1-Model_q4_058_12_conf0.93.png (46x85)\n",
      "Saved: PD637-A-2010-1-Model_q4_059_12_conf0.94.png (46x87)\n",
      "Saved: PD637-A-2010-1-Model_q4_060_12_conf0.95.png (46x86)\n",
      "Saved: PD637-A-2010-1-Model_q4_061_4_conf0.95.png (50x90)\n",
      "Saved: PD637-A-2010-1-Model_q4_062_4_conf0.95.png (49x88)\n",
      "Saved: PD637-A-2010-1-Model_q4_063_12_conf0.95.png (45x87)\n",
      "Saved: PD637-A-2010-1-Model_q4_064_12_conf0.95.png (46x87)\n",
      "Saved: PD637-A-2010-1-Model_q4_065_12_conf0.95.png (46x85)\n",
      "Saved: PD637-A-2010-1-Model_q4_066_12_conf0.95.png (44x86)\n",
      "Saved: PD637-A-2010-1-Model_q4_067_12_conf0.96.png (46x88)\n",
      "\n",
      "Processed 68 predictions\n",
      "Cropped images saved to: /home/peter/Desktop/yolov5/runs/detect/exp4\n",
      "{'top': 'PG', 'bottom': '20052'}\n",
      "{'top': 'PG', 'bottom': '20008'}\n",
      "{'top': 'LA', 'bottom': '20004'}\n",
      "{'top': 'FO', 'bottom': '20005'}\n",
      "{'top': 'PG', 'bottom': '20006'}\n",
      "{'top': 'NS', 'bottom': '20011'}\n",
      "{'top': 'LC', 'bottom': '20004'}\n",
      "{'top': 'NS', 'bottom': '20012'}\n",
      "{'tag': '@-a', 'type': 'Instrument_Field', 'center_x': 4344.0, 'center_y': 479.0}\n",
      "{'tag': 'PG-20051', 'type': 'Instrument_Field', 'center_x': 3684.5, 'center_y': 2550.5}\n",
      "{'tag': 'LG-20001', 'type': 'Instrument_Field', 'center_x': 3366.0, 'center_y': 3117.0}\n",
      "{'tag': 'PG-20052', 'type': 'Instrument_Field', 'center_x': 489.0, 'center_y': 300.0}\n",
      "{'tag': 'PG-20008', 'type': 'Instrument_Field', 'center_x': 1286.5, 'center_y': 2656.0}\n",
      "{'tag': 'LA-20004', 'type': 'Instrument_Panel', 'center_x': 1197.5, 'center_y': 515.0}\n",
      "{'tag': 'FO-20005', 'type': 'Instrument_Field', 'center_x': 1641.5, 'center_y': 832.5}\n",
      "{'tag': 'PG-20006', 'type': 'Instrument_Field', 'center_x': 1323.5, 'center_y': 1594.0}\n",
      "{'tag': 'NS-20011', 'type': 'Instrument_Panel', 'center_x': 2244.0, 'center_y': 2178.0}\n",
      "{'tag': 'LC-20004', 'type': 'Instrument_Panel', 'center_x': 969.5, 'center_y': 248.5}\n",
      "{'tag': 'NS-20012', 'type': 'Instrument_Panel', 'center_x': 2261.0, 'center_y': 3224.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inst = []\n",
    "#clean up previous runs\n",
    "detect_path = \"/home/peter/Desktop/yolov5/runs/detect\"\n",
    "try:\n",
    "    shutil.rmtree(detect_path)\n",
    "except:\n",
    "    pass\n",
    "#process_valves(valves,full_dwg,detect_path)\n",
    "process_valves_instruments(\"inst\",inst,full_dwg,detect_path)\n",
    "for n in inst:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb9a8c-99dd-4fd0-8968-3c6c5d219e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in inst:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60834738-1b87-4216-a8ac-4c627ac420b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run detect.py --weights ./best.pt --source /home/peter/dyn2_1.png --save-csv --imgsz 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f0014-2c5c-4892-a9b7-ce3d17c8dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run detect.py --weights ./best.pt --source /home/peter/ML101620329/ML101620329_q1.png --save-csv --imgsz 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4de40a-3da8-4cc0-8189-403f623e938c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2166660d-f82f-402d-92b5-e11c61f57936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
