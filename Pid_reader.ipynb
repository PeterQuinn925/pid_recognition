{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0fc9067-e714-46e0-bb69-e5156f8607eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (55929743.py, line 9)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install seaborn\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#install stuff\n",
    "# IMPORTANT NOTE: before starting Jupyter Notebook, navigate to /home/peter/Desktop/yolov5. Do source .venv/bin/activate\n",
    "# this sets up the virtual environment where torch is installed and allows yolo to work.\n",
    "# if installing this fresh, you'll need to install torch with Cuda (or not depending on the hardware)\n",
    "#Yolov5\n",
    "%cd /home/peter/Desktop/yolov5\n",
    "! source .venv/bin/activate\n",
    "pip install ultralytics\n",
    "pip install seaborn\n",
    "#pip install python>=3.1.30\n",
    "!pip install matplotlib>=3.3\n",
    "!pip install numpy>=1.23.5\n",
    "!pip install opencv-python>=4.1.1\n",
    "!pip install pillow>=10.3.0\n",
    "!pip install psutil  # system resources\n",
    "!pip install PyYAML>=5.3.1\n",
    "!pip install requests>=2.32.2\n",
    "!pip install scipy>=1.4.1\n",
    "!pip install hop>=0.1.1  # FLOPs computation\n",
    "!pip install torch>=1.8.0  # see https://pytorch.org/get-started/locally (recommended)\n",
    "!pip install torchvision>=0.9.0\n",
    "!pip install tqdm>=4.66.3\n",
    "!pip install ultralytics>=8.2.64  # https://ultralytics.com\n",
    "!pip install pandas>=1.1.4\n",
    "!pip install seaborn>=0.11.0\n",
    "# assume the files are in the right places. See https://github.com/ch-hristov/p-id-symbols for details. Follow the notebook\n",
    "#Ollama\n",
    "!pip install ollama\n",
    "#Claude\n",
    "!pip install anthropic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ecb1899-ec00-4726-9547-2c513502f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yolo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import ast\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import platform\n",
    "import sys\n",
    "# import torch ***this one is trouble\n",
    "\n",
    "#ollama\n",
    "import ollama\n",
    "import base64\n",
    "# import os duplicate\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "#Claude\n",
    "import anthropic\n",
    "\n",
    "#image handling\n",
    "import os\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import fitz  # PyMuPDF\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63268694-1e2f-4c93-a848-0f81539af88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anthropic key - do not publish\n",
    "key=\"sk-ant-api03redactedswAA\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d691c19-630d-42cf-8cc4-553f1685fbaa",
   "metadata": {},
   "source": [
    "order of operations\n",
    "1 Split the PDF into 4 quadrants\n",
    "2 Get the metadata from the titleblock in the bottom right quadrant\n",
    "3 Get the line list from a Claude query into a JSON string\n",
    "4 Get the valve list from Yolov5\n",
    "5 Go through each valve in the valve list and get the size and tag number via a png snippet around the valve. Include the coordinates of valve (include an offset for the quadrant). \n",
    "6 Get the line number of the valve, if possible, and add it to the valve list.\n",
    "7 Get an instrument list using the data from Yolov5. Get the tag name/number for each instrument and coordinates.\n",
    "8 Get an equipment list from a LLM (may need to use Claude)\n",
    "8 Use one of the LLMs to create a DEXPI file from the metatdata, line list, valve list, instrument list, and equipment list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d5810a-49f6-46d1-ba7e-e9d7d0695fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split P&ID into 4 quadrants\n",
    "\n",
    "def pdf_to_images(pdf_path, page_number=0, dpi=300):\n",
    "    \"\"\"\n",
    "    Convert a PDF page to a high-quality PIL Image\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        page_number (int): Page number to convert (0-indexed)\n",
    "        dpi (int): Resolution for the conversion\n",
    "    \n",
    "    Returns:\n",
    "        PIL.Image: The converted page as an image\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    if page_number >= len(doc):\n",
    "        raise ValueError(f\"Page {page_number} doesn't exist. PDF has {len(doc)} pages.\")\n",
    "    \n",
    "    page = doc[page_number]\n",
    "    \n",
    "    # Create a transformation matrix for higher resolution\n",
    "    mat = fitz.Matrix(dpi/72, dpi/72)  # 72 is the default DPI\n",
    "    \n",
    "    # Render the page as a pixmap\n",
    "    pix = page.get_pixmap(matrix=mat)\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    img_data = pix.tobytes(\"png\")\n",
    "    image = Image.open(io.BytesIO(img_data))\n",
    "    \n",
    "    doc.close()\n",
    "    return image\n",
    "\n",
    "def split_image_into_quadrants(image):\n",
    "    \"\"\"\n",
    "    Split a PIL Image into 4 equal quadrants\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image): The source image\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Four PIL Images representing the quadrants (top-left, top-right, bottom-left, bottom-right)\n",
    "    \"\"\"\n",
    "    width, height = image.size\n",
    "    \n",
    "    # Calculate quadrant dimensions\n",
    "    half_width = width // 2\n",
    "    half_height = height // 2\n",
    "    \n",
    "    # Define crop boxes for each quadrant\n",
    "    # (left, top, right, bottom)\n",
    "    top_left = (0, 0, half_width, half_height)\n",
    "    top_right = (half_width, 0, width, half_height)\n",
    "    bottom_left = (0, half_height, half_width, height)\n",
    "    bottom_right = (half_width, half_height, width, height)\n",
    "    \n",
    "    # Crop the image into quadrants\n",
    "    quadrants = (\n",
    "        image.crop(top_left),\n",
    "        image.crop(top_right),\n",
    "        image.crop(bottom_left),\n",
    "        image.crop(bottom_right)\n",
    "    )\n",
    "    \n",
    "    return quadrants\n",
    "\n",
    "def save_quadrants_as_png(quadrants, output_dir, base_filename):\n",
    "    \"\"\"\n",
    "    Save quadrant images as PNG files\n",
    "    \n",
    "    Args:\n",
    "        quadrants (tuple): Four PIL Images\n",
    "        output_dir (str): Directory to save the images\n",
    "        base_filename (str): Base name for the output files\n",
    "    \n",
    "    Returns:\n",
    "        list: Paths to the saved PNG files\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    quadrant_names = [\"top_left\", \"top_right\", \"bottom_left\", \"bottom_right\"]\n",
    "    saved_paths = []\n",
    "    \n",
    "    for i, (quadrant, name) in enumerate(zip(quadrants, quadrant_names)):\n",
    "        filename = f\"{base_filename}_q{i+1}.png\"\n",
    "#        filename = f\"{base_filename}_quadrant_{i+1}_{name}.png\"\n",
    "        filepath = output_dir / filename\n",
    "        \n",
    "        # Save as PNG with high quality\n",
    "        quadrant.save(filepath, \"PNG\", optimize=True)\n",
    "        saved_paths.append(str(filepath))\n",
    "#        print(f\"Saved quadrant {i+1} ({name}): {filepath}\")\n",
    "    \n",
    "    return saved_paths\n",
    "    \n",
    "def create_quads(pdf_path,page,output_dir):\n",
    "     image = pdf_to_images(pdf_path, page)\n",
    "     quadrants = split_image_into_quadrants(image)\n",
    "# Save quadrants as PNG files\n",
    "     base_filename = Path(pdf_path).stem\n",
    "     quadrant_paths = save_quadrants_as_png(quadrants, output_dir, base_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f604a422-9323-439d-a9f7-bbf0047b5969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call Ollama LLMs. Encode PNGs too.\n",
    "def encode_png_file(png_path: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Encode a PNG file to base64\n",
    "    \n",
    "    Args:\n",
    "        png_path (str): Path to the PNG file\n",
    "    \n",
    "    Returns:\n",
    "        str: Base64 encoded image data, or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(png_path, 'rb') as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding {png_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_png_files(directory: str, pattern: str = \"*.png\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Find all PNG files in a directory\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Directory to search\n",
    "        pattern (str): File pattern (default: \"*.png\")\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: List of PNG file paths\n",
    "    \"\"\"\n",
    "    search_path = os.path.join(directory, pattern)\n",
    "    png_files = glob.glob(search_path)\n",
    "    png_files.sort()  # Sort for consistent ordering\n",
    "    return png_files\n",
    "\n",
    "def query_ollama(model, prompt, png_path):\n",
    "    if png_path != \"\": \n",
    "        encoded_image = encode_png_file(png_path)\n",
    "    \n",
    "        response = ollama.chat(\n",
    "           model=model,\n",
    "           messages=[{'role': 'user', 'content': prompt,'images': [encoded_image]}],\n",
    "           options={\n",
    "               'temperature': 0.7,  # Range: 0.0 to 1.0\n",
    "           }\n",
    "        )\n",
    "    else:\n",
    "        response = ollama.chat(\n",
    "           model=model,\n",
    "           messages=[{'role': 'user', 'content': prompt}],\n",
    "           options={\n",
    "               'temperature': 0.7,  # Range: 0.0 to 1.0\n",
    "           }\n",
    "        )\n",
    "\n",
    "    foo = response.message.content[8:-4]\n",
    "    #print(response.message.content)\n",
    "    response_J = json.loads(foo)\n",
    "    return response_J\n",
    "\n",
    "def query_ollama_multiple(model, prompt, png_path): #same as above, except png_path is a list of pngs\n",
    "    #!!!!!This does not work. Right now ollama does not support multiple images\n",
    "    encoded_images = []\n",
    "    \n",
    "    for i,png in enumerate(png_path):\n",
    "       encoded_images.append(encode_png_file(png))\n",
    "   \n",
    "    response = ollama.chat(\n",
    "       model=model,\n",
    "       messages=[{'role': 'user', 'content': prompt,'images': [encoded_images]}],\n",
    "       options={\n",
    "           'temperature': 0.7,  # Range: 0.0 to 1.0\n",
    "       }\n",
    "    )\n",
    "\n",
    "    foo = response.message.content[8:-4]\n",
    "    response_J = json.loads(foo)\n",
    "    return response_J    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "034780ff-bc32-417f-9c1e-35a47ea5a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Claude\n",
    "Claude = \"claude-sonnet-4-20250514\"\n",
    "\n",
    "#initialize\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=key  # or set ANTHROPIC_API_KEY environment variable\n",
    ")\n",
    "\n",
    "def claude_simple(prompt_text):\n",
    "    content = []\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": prompt_text\n",
    "    })\n",
    "    \n",
    "    message = client.messages.create(\n",
    "        model=Claude,\n",
    "        max_tokens=2000,\n",
    "        temperature = 0.5,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content\n",
    "        }]\n",
    "    )\n",
    "    return message.content[0].text \n",
    "\n",
    "def upload_multiple_images_to_claude(image_paths, prompt_text):\n",
    "    \"\"\"\n",
    "    Upload multiple images to Claude\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): List of paths to image files\n",
    "        prompt_text (str): Text prompt to send along with the images\n",
    "    \n",
    "    Returns:\n",
    "        str: Claude's response\n",
    "    \"\"\"\n",
    "    \n",
    "    content = []\n",
    "    \n",
    "    # Add all images\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        image_path = Path(image_path)\n",
    "        \n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "        \n",
    "        content.append({\n",
    "            \"type\": \"image\",\n",
    "            \"source\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": \"image/png\",\n",
    "                \"data\": image_data\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Add text prompt\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": prompt_text\n",
    "    })\n",
    "    \n",
    "    message = client.messages.create(\n",
    "        model=Claude,\n",
    "        max_tokens=2000,\n",
    "        temperature = 0.5,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6daefdd6-baa8-47cc-9510-df6afd5d7749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping YOLO predictions\n",
    "def crop_predictions(png_file_path, predictions_csv_path, scaling_factor=1.0, output_dir=\"cropped_images\"):\n",
    "    \"\"\"\n",
    "    Crop image regions based on predictions CSV with bounding box coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    png_file_path (str): Path to the input PNG image\n",
    "    predictions_csv_path (str): Path to the predictions CSV file\n",
    "    scaling_factor (float): Factor to scale the bounding box size (1.0 = original size)\n",
    "    output_dir (str): Directory to save cropped images\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(predictions_csv_path)\n",
    "    \n",
    "    # Load the image\n",
    "    image = Image.open(png_file_path)\n",
    "    image_width, image_height = image.size\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get the base name of the PNG file (without extension)\n",
    "    base_name = Path(png_file_path).stem\n",
    "    \n",
    "    cropped_images = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Parse the xyxy coordinates from string representation\n",
    "        # Remove 'tensor()' wrapper and convert to float\n",
    "        coords_str = row['Xyxy'].strip('[]')\n",
    "        coords_parts = coords_str.split(', ')\n",
    "        \n",
    "        # Extract numerical values from tensor format\n",
    "        coords = []\n",
    "        for part in coords_parts:\n",
    "            # Remove 'tensor(' and ')' and convert to float\n",
    "            num_str = part.strip().replace('tensor(', '').replace(')', '').replace('.', '')\n",
    "            coords.append(float(num_str))\n",
    "        \n",
    "        x1, y1, x2, y2 = coords\n",
    "        \n",
    "        # Calculate center and dimensions of bounding box\n",
    "        center_x = (x1 + x2) / 2\n",
    "        center_y = (y1 + y2) / 2\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        \n",
    "        # Apply scaling factor\n",
    "        scaled_width = width * scaling_factor\n",
    "        scaled_height = height * scaling_factor\n",
    "        \n",
    "        # Calculate new coordinates\n",
    "        new_x1 = center_x - scaled_width / 2\n",
    "        new_y1 = center_y - scaled_height / 2\n",
    "        new_x2 = center_x + scaled_width / 2\n",
    "        new_y2 = center_y + scaled_height / 2\n",
    "        \n",
    "        # Ensure coordinates are within image bounds\n",
    "        new_x1 = max(0, min(new_x1, image_width))\n",
    "        new_y1 = max(0, min(new_y1, image_height))\n",
    "        new_x2 = max(0, min(new_x2, image_width))\n",
    "        new_y2 = max(0, min(new_y2, image_height))\n",
    "        \n",
    "        # Crop the image\n",
    "        cropped_img = image.crop((new_x1, new_y1, new_x2, new_y2))\n",
    "        \n",
    "        # Create filename for cropped image\n",
    "        prediction_clean = row['Prediction']\n",
    "        confidence = f\"{row['Confidence']:.2f}\"\n",
    "        output_filename = f\"{base_name}_{idx:03d}_{prediction_clean}_conf{confidence}.png\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "        # Save cropped image\n",
    "        cropped_img.save(output_path)\n",
    "        \n",
    "        cropped_images.append({\n",
    "            'index': idx,\n",
    "            'prediction': row['Prediction'],\n",
    "            'confidence': row['Confidence'],\n",
    "            'original_coords': (x1, y1, x2, y2),\n",
    "            'scaled_coords': (new_x1, new_y1, new_x2, new_y2),\n",
    "            'output_path': output_path,\n",
    "            'size': cropped_img.size\n",
    "        })\n",
    "        \n",
    "        print(f\"Saved: {output_filename} ({cropped_img.size[0]}x{cropped_img.size[1]})\")\n",
    "    \n",
    "    print(f\"\\nProcessed {len(cropped_images)} predictions\")\n",
    "    print(f\"Cropped images saved to: {output_dir}\")\n",
    "    \n",
    "    return cropped_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5aa6f80f-df86-4887-b5ab-c75aff351dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' classes\\n   1: Gate_Valve\\n    2: Ball_Valve\\n    3: Globe_valve_NO\\n    4: Gate_valve_NO\\n    5: Globe_valve_NO\\n    6: Butterfly_valve\\n    7: Plug valve\\n    8: Check_valve\\n    9: Diaphragm_valve\\n    10: Needle_valve\\n    11: Half_Filled_Gate_Valve\\n    12: Gate_Valve_NC\\n    13: Globle_valve_NC\\n    14: Control_Valve\\n    15: Rotary_Valve\\n    16: Ball_valve_NC\\n    17: Paddle_blind\\n    18: Spectacle_blind_Closed\\n    19: Spectacle_blind_Open\\n    20: Reducer\\n    21: Flange_or_Nozzle\\n    22: Rupture_disk\\n    23: Pipe_Insulation_or_Tracing\\n    24: Flow_Arrow\\n    25: sight_glass\\n    26: Instrument_Field\\n    27: Instrument_Field\\n    28: Instrument_Panel\\n    29: Instrument_Aux_Panel\\n    30: box\\n    31: Instrument_Panel\\n    32: box\\n    '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def class_lookup(prediction):\n",
    "    match prediction:\n",
    "        case 1:\n",
    "            res = \"Gate_Valve\"\n",
    "        case 2:\n",
    "            res = \"Ball_Valve\"\n",
    "        case 3:\n",
    "            res = \"Globe_valve_NO\"\n",
    "        case 4: \n",
    "            res = \"Gate_valve_NO\"\n",
    "        case 5: \n",
    "            res = \"Globe_valve_NO\"\n",
    "        case 6: \n",
    "            res = \"Butterfly_Valve\"\n",
    "        case 7: \n",
    "            res = \"Plug Valve\"\n",
    "        case 8: \n",
    "            res = \"Check_Valve\"\n",
    "        case 9: \n",
    "            res = \"Diaphragm_valve\"\n",
    "        case 10: \n",
    "            res = \"Needle_valve\"\n",
    "        case 11: \n",
    "            res = \"Half_Filled_Gate_Valve\"\n",
    "        case 12: \n",
    "            res = \"Gate_Valve_NC\"\n",
    "        case 13: \n",
    "            res = \"Globle_valve_NC\"\n",
    "        case 14: \n",
    "            res = \"Control_Valve\"\n",
    "        case 15:\n",
    "            res = \"Rotary_Valve\"\n",
    "        case 16: \n",
    "            res = \"Ball_valve_NC\"\n",
    "# probably should do the rest too, but enough for now...\n",
    "    return res\n",
    "\n",
    "\"\"\" classes\n",
    "   1: Gate_Valve\n",
    "    2: Ball_Valve\n",
    "    3: Globe_valve_NO\n",
    "    4: Gate_valve_NO\n",
    "    5: Globe_valve_NO\n",
    "    6: Butterfly_valve\n",
    "    7: Plug valve\n",
    "    8: Check_valve\n",
    "    9: Diaphragm_valve\n",
    "    10: Needle_valve\n",
    "    11: Half_Filled_Gate_Valve\n",
    "    12: Gate_Valve_NC\n",
    "    13: Globle_valve_NC\n",
    "    14: Control_Valve\n",
    "    15: Rotary_Valve\n",
    "    16: Ball_valve_NC\n",
    "    17: Paddle_blind\n",
    "    18: Spectacle_blind_Closed\n",
    "    19: Spectacle_blind_Open\n",
    "    20: Reducer\n",
    "    21: Flange_or_Nozzle\n",
    "    22: Rupture_disk\n",
    "    23: Pipe_Insulation_or_Tracing\n",
    "    24: Flow_Arrow\n",
    "    25: sight_glass\n",
    "    26: Instrument_Field\n",
    "    27: Instrument_Field\n",
    "    28: Instrument_Panel\n",
    "    29: Instrument_Aux_Panel\n",
    "    30: box\n",
    "    31: Instrument_Panel\n",
    "    32: box\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d469be14-448c-42d3-b2b2-f10c124ee351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_valves(valves,full_dwg,detect_path):\n",
    "    for quad in range(4):\n",
    "        #image size is important - if the size is too far off, detection is not accurate\n",
    "        run_path = '\"./detect.py\" --weights \"./best.pt\" --source '+full_dwg[quad]+' --conf-thres 0.5 --save-csv --imgsz 5000'\n",
    "        %run {run_path}\n",
    "        result_path = detect_path+\"/exp\"\n",
    "        #create cropped images\n",
    "        crop_predictions(full_dwg[quad], result_path+\"/predictions.csv\", scaling_factor=10.0, output_dir=result_path)\n",
    "        #go through each detected item and add the ones we want to the valve list\n",
    "            # Read the CSV file\n",
    "        df = pd.read_csv(result_path+\"/predictions.csv\")\n",
    "        base_name = Path(full_dwg[quad]).stem\n",
    "        for idx, row in df.iterrows():\n",
    "            # Parse the xyxy coordinates from string representation\n",
    "            # Remove 'tensor()' wrapper and convert to float\n",
    "            coords_str = row['Xyxy'].strip('[]')\n",
    "            coords_parts = coords_str.split(', ')\n",
    "            prediction = row['Prediction']\n",
    "            #Prediction holds an int containing the type of item found. We only want certain types\n",
    "            if prediction > 0 and prediction <=16: # this is a valve\n",
    "               classname = class_lookup(prediction)\n",
    "               # Extract numerical values from tensor format to get the valve center coordinates\n",
    "               coords = []\n",
    "               for part in coords_parts:\n",
    "               # Remove 'tensor(' and ')' and convert to float\n",
    "                   num_str = part.strip().replace('tensor(', '').replace(')', '').replace('.', '')\n",
    "                   coords.append(float(num_str))\n",
    "               x1, y1, x2, y2 = coords\n",
    "               # Calculate center and dimensions of bounding box\n",
    "               center_x = (x1 + x2) / 2\n",
    "               center_y = (y1 + y2) / 2\n",
    "               #Get the image filename and send it to a LLM to get the size and tag number\n",
    "               # reCreate filename for cropped image\n",
    "               prediction_clean = row['Prediction']\n",
    "               confidence = f\"{row['Confidence']:.2f}\"\n",
    "               output_filename = f\"{base_name}_{idx:03d}_{prediction_clean}_conf{confidence}.png\"\n",
    "               png_path = os.path.join(result_path, output_filename)\n",
    "               prompt = \"\"\"Look at the valve in the center of the image. Return the valve size and valve tag number as a \n",
    "               json string. If either value can't be found return the value as none\"\"\"\n",
    "               result = query_ollama(ollama_model, prompt, png_path)\n",
    "               print(result)\n",
    "               valve_size = result[0]\n",
    "               valve_tag = result[1]\n",
    "#FUTURE: try to get the line number of the valve too\n",
    "               valvedict = {\"tag\": valve_tag, \"type\": classname, \"size\": valve_size, \"center_x\": center_x, \"center_y\": center_y}\n",
    "               valve.append(valvedict)\n",
    "    return valves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fce5e59e-8b38-4230-b326-b86306b87986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['./best.pt'], source=/home/peter/test3/test3_q1.png, data=data/coco128.yaml, imgsz=[5000, 5000], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_format=0, save_csv=True, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['gitpython>=3.1.30'] not found, attempting AutoUpdate...\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 0.0s\n",
      "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 v7.0-430-g459d8bf0 Python-3.12.3 torch-2.8.0+cu128 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 267 layers, 46280598 parameters, 0 gradients, 108.2 GFLOPs\n",
      "WARNING ⚠️ --img-size [5000, 5000] must be multiple of max stride 32, updating to [5024, 5024]\n",
      "image 1/1 /home/peter/test3/test3_q1.png: 3552x5024 7 4s, 9 12s, 3 13s, 2 19s, 1 20, 1 21, 9 23s, 9 24s, 3 26s, 2 32s, 8254.4ms\n",
      "Speed: 14.4ms pre-process, 8254.4ms inference, 6.4ms NMS per image at shape (1, 3, 5024, 5024)\n",
      "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: test3_q1_000_32_conf0.56.png (1160x530)\n",
      "Saved: test3_q1_001_24_conf0.63.png (770x380)\n",
      "Saved: test3_q1_002_13_conf0.63.png (149x410)\n",
      "Saved: test3_q1_003_24_conf0.64.png (410x610)\n",
      "Saved: test3_q1_004_4_conf0.64.png (830x450)\n",
      "Saved: test3_q1_005_26_conf0.65.png (340x400)\n",
      "Saved: test3_q1_006_26_conf0.66.png (1570x1630)\n",
      "Saved: test3_q1_007_21_conf0.67.png (290x160)\n",
      "Saved: test3_q1_008_23_conf0.68.png (1170x500)\n",
      "Saved: test3_q1_009_26_conf0.68.png (1560x1201)\n",
      "Saved: test3_q1_010_24_conf0.69.png (730x420)\n",
      "Saved: test3_q1_011_24_conf0.72.png (400x680)\n",
      "Saved: test3_q1_012_24_conf0.72.png (670x600)\n",
      "Saved: test3_q1_013_24_conf0.72.png (670x610)\n",
      "Saved: test3_q1_014_13_conf0.73.png (210x420)\n",
      "Saved: test3_q1_015_24_conf0.75.png (400x650)\n",
      "Saved: test3_q1_016_32_conf0.75.png (1190x520)\n",
      "Saved: test3_q1_017_4_conf0.75.png (810x440)\n",
      "Saved: test3_q1_018_23_conf0.77.png (1170x500)\n",
      "Saved: test3_q1_019_20_conf0.82.png (460x780)\n",
      "Saved: test3_q1_020_12_conf0.82.png (420x780)\n",
      "Saved: test3_q1_021_24_conf0.82.png (710x380)\n",
      "Saved: test3_q1_022_23_conf0.84.png (1210x510)\n",
      "Saved: test3_q1_023_4_conf0.84.png (820x430)\n",
      "Saved: test3_q1_024_4_conf0.85.png (800x450)\n",
      "Saved: test3_q1_025_24_conf0.85.png (680x400)\n",
      "Saved: test3_q1_026_23_conf0.86.png (1170x510)\n",
      "Saved: test3_q1_027_4_conf0.86.png (800x450)\n",
      "Saved: test3_q1_028_13_conf0.86.png (210x410)\n",
      "Saved: test3_q1_029_23_conf0.86.png (1180x520)\n",
      "Saved: test3_q1_030_23_conf0.86.png (1180x520)\n",
      "Saved: test3_q1_031_23_conf0.87.png (1140x510)\n",
      "Saved: test3_q1_032_23_conf0.87.png (1160x520)\n",
      "Saved: test3_q1_033_23_conf0.88.png (1170x510)\n",
      "Saved: test3_q1_034_19_conf0.89.png (1150x440)\n",
      "Saved: test3_q1_035_4_conf0.89.png (800x440)\n",
      "Saved: test3_q1_036_12_conf0.89.png (790x410)\n",
      "Saved: test3_q1_037_12_conf0.90.png (790x400)\n",
      "Saved: test3_q1_038_12_conf0.90.png (800x400)\n",
      "Saved: test3_q1_039_12_conf0.90.png (450x810)\n",
      "Saved: test3_q1_040_19_conf0.91.png (450x1190)\n",
      "Saved: test3_q1_041_4_conf0.94.png (460x810)\n",
      "Saved: test3_q1_042_12_conf0.94.png (430x770)\n",
      "Saved: test3_q1_043_12_conf0.94.png (420x790)\n",
      "Saved: test3_q1_044_12_conf0.95.png (420x770)\n",
      "Saved: test3_q1_045_12_conf0.95.png (420x780)\n",
      "\n",
      "Processed 46 predictions\n",
      "Cropped images saved to: /home/peter/Desktop/yolov5/runs/detect/exp\n",
      "{'valve_size': 'none', 'valve_tag_number': '3'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m shutil.rmtree(detect_path)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# go through each prediction and add the right info to the valve list\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mprocess_valves\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalves\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfull_dwg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdetect_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mprocess_valves\u001b[39m\u001b[34m(valves, full_dwg, detect_path)\u001b[39m\n\u001b[32m     40\u001b[39m                result = query_ollama(ollama_model, prompt, png_path)\n\u001b[32m     41\u001b[39m                \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m                valve_size = \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     43\u001b[39m                valve_tag = result[\u001b[32m1\u001b[39m]\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m#FUTURE: try to get the line number of the valve too\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 0"
     ]
    }
   ],
   "source": [
    "#main\n",
    "pdf_file = \"/home/peter/test3.pdf\"\n",
    "page = 0\n",
    "ollama_model = 'gemma3:27b'\n",
    "\n",
    "output_dir = pdf_file[:-4]\n",
    "filename = os.path.splitext(os.path.basename(pdf_file))[0]\n",
    "create_quads(pdf_file,page,output_dir) #create PNG files\n",
    "\n",
    "# Call Gemma3:27b via Ollama to get metadata\n",
    "prompt = \"\"\"List the project name, company name, drawing (DWG) name, drawing number, location, date, revision, author, \n",
    "and any other information found in the title block in the lower right corner of the image. Return data as JSON.\"\"\"\n",
    "png_path = output_dir+\"/\"+filename+\"_q4.png\"\n",
    "drawing_info = query_ollama(ollama_model, prompt, png_path\n",
    "                           )\n",
    "# Call Claude and get line list. Need to send all 4 quadrants\n",
    "full_dwg = [] #all 4 quadrants\n",
    "for i in range(4):\n",
    "    n = i+1\n",
    "    full_dwg.append(output_dir+\"/\"+filename+\"_q\"+str(n)+\".png\")\n",
    "prompt = \"\"\"list the pipe lines on this P&ID with the line number, primary size, and what equipment, off-page connectors, \n",
    "or other lines it is connected to. Return the data as JSON and only return JSON.\"\"\"\n",
    "result = upload_multiple_images_to_claude(full_dwg, prompt)\n",
    "line_list = json.loads(result[8:-4])\n",
    "\n",
    "#equipment list\n",
    "prompt = \"\"\"List the equipment on this P&ID with the name, description and associated metadata. Only list the primary equipment. \n",
    "Do not list piping, valves, instruments or information about the drawing itself. Return data as JSON\"\"\"\n",
    "result = upload_multiple_images_to_claude(full_dwg, prompt)\n",
    "equip_list = json.loads(result[8:-4])\n",
    "\n",
    "#valves\n",
    "valves = []\n",
    "#clean up previous runs\n",
    "detect_path = \"/home/peter/Desktop/yolov5/runs/detect\"\n",
    "shutil.rmtree(detect_path)\n",
    "# go through each prediction and add the right info to the valve list\n",
    "process_valves(valves,full_dwg,detect_path)\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c7ea4-2483-4ee3-ac49-475e99e96b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ca1f864-83c4-4dc9-9895-2f6d17e8a17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['./best.pt'], source=/home/peter/test3/test3_q1.png, data=data/coco128.yaml, imgsz=[5000, 5000], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_format=0, save_csv=True, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['gitpython>=3.1.30'] not found, attempting AutoUpdate...\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 0.0s\n",
      "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 v7.0-430-g459d8bf0 Python-3.12.3 torch-2.8.0+cu128 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 267 layers, 46280598 parameters, 0 gradients, 108.2 GFLOPs\n",
      "WARNING ⚠️ --img-size [5000, 5000] must be multiple of max stride 32, updating to [5024, 5024]\n",
      "image 1/1 /home/peter/test3/test3_q1.png: 3552x5024 7 4s, 9 12s, 3 13s, 2 19s, 1 20, 1 21, 9 23s, 9 24s, 3 26s, 2 32s, 8921.0ms\n",
      "Speed: 19.1ms pre-process, 8921.0ms inference, 15.9ms NMS per image at shape (1, 3, 5024, 5024)\n",
      "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: test3_q1_000_32_conf0.56.png (1160x530)\n",
      "Saved: test3_q1_001_24_conf0.63.png (770x380)\n",
      "Saved: test3_q1_002_13_conf0.63.png (149x410)\n",
      "Saved: test3_q1_003_24_conf0.64.png (410x610)\n",
      "Saved: test3_q1_004_4_conf0.64.png (830x450)\n",
      "Saved: test3_q1_005_26_conf0.65.png (340x400)\n",
      "Saved: test3_q1_006_26_conf0.66.png (1570x1630)\n",
      "Saved: test3_q1_007_21_conf0.67.png (290x160)\n",
      "Saved: test3_q1_008_23_conf0.68.png (1170x500)\n",
      "Saved: test3_q1_009_26_conf0.68.png (1560x1201)\n",
      "Saved: test3_q1_010_24_conf0.69.png (730x420)\n",
      "Saved: test3_q1_011_24_conf0.72.png (400x680)\n",
      "Saved: test3_q1_012_24_conf0.72.png (670x600)\n",
      "Saved: test3_q1_013_24_conf0.72.png (670x610)\n",
      "Saved: test3_q1_014_13_conf0.73.png (210x420)\n",
      "Saved: test3_q1_015_24_conf0.75.png (400x650)\n",
      "Saved: test3_q1_016_32_conf0.75.png (1190x520)\n",
      "Saved: test3_q1_017_4_conf0.75.png (810x440)\n",
      "Saved: test3_q1_018_23_conf0.77.png (1170x500)\n",
      "Saved: test3_q1_019_20_conf0.82.png (460x780)\n",
      "Saved: test3_q1_020_12_conf0.82.png (420x780)\n",
      "Saved: test3_q1_021_24_conf0.82.png (710x380)\n",
      "Saved: test3_q1_022_23_conf0.84.png (1210x510)\n",
      "Saved: test3_q1_023_4_conf0.84.png (820x430)\n",
      "Saved: test3_q1_024_4_conf0.85.png (800x450)\n",
      "Saved: test3_q1_025_24_conf0.85.png (680x400)\n",
      "Saved: test3_q1_026_23_conf0.86.png (1170x510)\n",
      "Saved: test3_q1_027_4_conf0.86.png (800x450)\n",
      "Saved: test3_q1_028_13_conf0.86.png (210x410)\n",
      "Saved: test3_q1_029_23_conf0.86.png (1180x520)\n",
      "Saved: test3_q1_030_23_conf0.86.png (1180x520)\n",
      "Saved: test3_q1_031_23_conf0.87.png (1140x510)\n",
      "Saved: test3_q1_032_23_conf0.87.png (1160x520)\n",
      "Saved: test3_q1_033_23_conf0.88.png (1170x510)\n",
      "Saved: test3_q1_034_19_conf0.89.png (1150x440)\n",
      "Saved: test3_q1_035_4_conf0.89.png (800x440)\n",
      "Saved: test3_q1_036_12_conf0.89.png (790x410)\n",
      "Saved: test3_q1_037_12_conf0.90.png (790x400)\n",
      "Saved: test3_q1_038_12_conf0.90.png (800x400)\n",
      "Saved: test3_q1_039_12_conf0.90.png (450x810)\n",
      "Saved: test3_q1_040_19_conf0.91.png (450x1190)\n",
      "Saved: test3_q1_041_4_conf0.94.png (460x810)\n",
      "Saved: test3_q1_042_12_conf0.94.png (430x770)\n",
      "Saved: test3_q1_043_12_conf0.94.png (420x790)\n",
      "Saved: test3_q1_044_12_conf0.95.png (420x770)\n",
      "Saved: test3_q1_045_12_conf0.95.png (420x780)\n",
      "\n",
      "Processed 46 predictions\n",
      "Cropped images saved to: /home/peter/Desktop/yolov5/runs/detect/exp\n",
      "/home/peter/Desktop/yolov5/runs/detect/exp/test3_q1_002_13_conf0.63.png\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m detect_path = \u001b[33m\"\u001b[39m\u001b[33m/home/peter/Desktop/yolov5/runs/detect\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m shutil.rmtree(detect_path)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mprocess_valves\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalves\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfull_dwg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdetect_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(valves)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mprocess_valves\u001b[39m\u001b[34m(valves, full_dwg, detect_path)\u001b[39m\n\u001b[32m     38\u001b[39m                prompt = \u001b[33m\"\"\"\u001b[39m\u001b[33mLook at the valve in the center of the image. Return the valve size and valve tag number as a \u001b[39m\n\u001b[32m     39\u001b[39m \u001b[33m               comma separated string and only the comma separated string. If either value can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be found return the value as none\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     40\u001b[39m                \u001b[38;5;28mprint\u001b[39m(png_path)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m                result = \u001b[43mquery_ollama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mollama_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpng_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m#TODO: need a try/catch here for oddballs\u001b[39;00m\n\u001b[32m     43\u001b[39m                result_list = result.split(\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mquery_ollama\u001b[39m\u001b[34m(model, prompt, png_path)\u001b[39m\n\u001b[32m     55\u001b[39m foo = response.message.content[\u001b[32m8\u001b[39m:-\u001b[32m4\u001b[39m]\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m#print(response.message.content)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m response_J = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfoo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response_J\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    333\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     end = _w(s, end).end()\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/json/decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    353\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "valves = []\n",
    "#clean up previous runs\n",
    "detect_path = \"/home/peter/Desktop/yolov5/runs/detect\"\n",
    "shutil.rmtree(detect_path)\n",
    "\n",
    "process_valves(valves,full_dwg,detect_path)\n",
    "print(valves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06cb9a8c-99dd-4fd0-8968-3c6c5d219e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/peter/test3/test3_q1.png', '/home/peter/test3/test3_q2.png', '/home/peter/test3/test3_q3.png', '/home/peter/test3/test3_q4.png']\n"
     ]
    }
   ],
   "source": [
    "print (full_dwg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60834738-1b87-4216-a8ac-4c627ac420b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
