{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc9067-e714-46e0-bb69-e5156f8607eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install stuff\n",
    "# IMPORTANT NOTE: before starting Jupyter Notebook, navigate to /home/peter/Desktop/yolov5. Do source .venv/bin/activate\n",
    "# this sets up the virtual environment where torch is installed and allows yolo to work.\n",
    "# if installing this fresh, you'll need to install torch with Cuda (or not depending on the hardware)\n",
    "#Yolov5\n",
    "%cd /home/peter/Desktop/yolov5\n",
    "!source .venv/bin/activate\n",
    "#pip install python>=3.1.30\n",
    "!pip install matplotlib>=3.3\n",
    "!pip install numpy>=1.23.5\n",
    "!pip install opencv-python>=4.1.1\n",
    "!pip install pillow>=10.3.0\n",
    "!pip install psutil  # system resources\n",
    "!pip install PyYAML>=5.3.1\n",
    "!pip install requests>=2.32.2\n",
    "!pip install scipy>=1.4.1\n",
    "!pip install hop>=0.1.1  # FLOPs computation\n",
    "!pip install torch>=1.8.0  # see https://pytorch.org/get-started/locally (recommended)\n",
    "!pip install torchvision>=0.9.0\n",
    "!pip install tqdm>=4.66.3\n",
    "!pip install ultralytics>=8.2.64  # https://ultralytics.com\n",
    "!pip install pandas>=1.1.4\n",
    "!pip install seaborn>=0.11.0\n",
    "# assume the files are in the right places. See https://github.com/ch-hristov/p-id-symbols for details. Follow the notebook\n",
    "#Ollama\n",
    "!pip install ollama\n",
    "#Claude\n",
    "!pip install anthropic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9ecb1899-ec00-4726-9547-2c513502f327",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[116]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplatform\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#ollama\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mollama\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "#yolo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import ast\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import platform\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "#ollama\n",
    "import ollama\n",
    "import base64\n",
    "# import os duplicate\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "#Claude\n",
    "import anthropic\n",
    "\n",
    "#image handling\n",
    "import os\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import fitz  # PyMuPDF\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63268694-1e2f-4c93-a848-0f81539af88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anthropic key - do not publish\n",
    "key=\"slongstuffffhereeeeeee(*&^%$^&%$%^^&GHGGHNBbvhdhhA\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d691c19-630d-42cf-8cc4-553f1685fbaa",
   "metadata": {},
   "source": [
    "order of operations\n",
    "1 Split the PDF into 4 quadrants\n",
    "2 Get the metadata from the titleblock in the bottom right quadrant\n",
    "3 Get the line list from a Claude query into a JSON string\n",
    "4 Get the valve list from Yolov5\n",
    "5 Go through each valve in the valve list and get the size and tag number via a png snippet around the valve. Include the coordinates of valve (include an offset for the quadrant). \n",
    "6 Get the line number of the valve, if possible, and add it to the valve list.\n",
    "7 Get an instrument list using the data from Yolov5. Get the tag name/number for each instrument and coordinates.\n",
    "8 Get an equipment list from a LLM (may need to use Claude)\n",
    "8 Use one of the LLMs to create a DEXPI file from the metatdata, line list, valve list, instrument list, and equipment list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "32d5810a-49f6-46d1-ba7e-e9d7d0695fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split P&ID into 4 quadrants\n",
    "\n",
    "def pdf_to_images(pdf_path, page_number=0, dpi=300):\n",
    "    \"\"\"\n",
    "    Convert a PDF page to a high-quality PIL Image\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        page_number (int): Page number to convert (0-indexed)\n",
    "        dpi (int): Resolution for the conversion\n",
    "    \n",
    "    Returns:\n",
    "        PIL.Image: The converted page as an image\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    if page_number >= len(doc):\n",
    "        raise ValueError(f\"Page {page_number} doesn't exist. PDF has {len(doc)} pages.\")\n",
    "    \n",
    "    page = doc[page_number]\n",
    "    \n",
    "    # Create a transformation matrix for higher resolution\n",
    "    mat = fitz.Matrix(dpi/72, dpi/72)  # 72 is the default DPI\n",
    "    \n",
    "    # Render the page as a pixmap\n",
    "    pix = page.get_pixmap(matrix=mat)\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    img_data = pix.tobytes(\"png\")\n",
    "    image = Image.open(io.BytesIO(img_data))\n",
    "    \n",
    "    doc.close()\n",
    "    return image\n",
    "\n",
    "def split_image_into_quadrants(image):\n",
    "    \"\"\"\n",
    "    Split a PIL Image into 4 equal quadrants\n",
    "    \n",
    "    Args:\n",
    "        image (PIL.Image): The source image\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Four PIL Images representing the quadrants (top-left, top-right, bottom-left, bottom-right)\n",
    "    \"\"\"\n",
    "    width, height = image.size\n",
    "    \n",
    "    # Calculate quadrant dimensions\n",
    "    half_width = width // 2\n",
    "    half_height = height // 2\n",
    "    \n",
    "    # Define crop boxes for each quadrant\n",
    "    # (left, top, right, bottom)\n",
    "    top_left = (0, 0, half_width, half_height)\n",
    "    top_right = (half_width, 0, width, half_height)\n",
    "    bottom_left = (0, half_height, half_width, height)\n",
    "    bottom_right = (half_width, half_height, width, height)\n",
    "    \n",
    "    # Crop the image into quadrants\n",
    "    quadrants = (\n",
    "        image.crop(top_left),\n",
    "        image.crop(top_right),\n",
    "        image.crop(bottom_left),\n",
    "        image.crop(bottom_right)\n",
    "    )\n",
    "    \n",
    "    return quadrants\n",
    "\n",
    "def save_quadrants_as_png(quadrants, output_dir, base_filename):\n",
    "    \"\"\"\n",
    "    Save quadrant images as PNG files\n",
    "    \n",
    "    Args:\n",
    "        quadrants (tuple): Four PIL Images\n",
    "        output_dir (str): Directory to save the images\n",
    "        base_filename (str): Base name for the output files\n",
    "    \n",
    "    Returns:\n",
    "        list: Paths to the saved PNG files\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    quadrant_names = [\"top_left\", \"top_right\", \"bottom_left\", \"bottom_right\"]\n",
    "    saved_paths = []\n",
    "    \n",
    "    for i, (quadrant, name) in enumerate(zip(quadrants, quadrant_names)):\n",
    "        filename = f\"{base_filename}_q{i+1}.png\"\n",
    "#        filename = f\"{base_filename}_quadrant_{i+1}_{name}.png\"\n",
    "        filepath = output_dir / filename\n",
    "        \n",
    "        # Save as PNG with high quality\n",
    "        quadrant.save(filepath, \"PNG\", optimize=True)\n",
    "        saved_paths.append(str(filepath))\n",
    "#        print(f\"Saved quadrant {i+1} ({name}): {filepath}\")\n",
    "    \n",
    "    return saved_paths\n",
    "    \n",
    "def create_quads(pdf_path,page,output_dir):\n",
    "     image = pdf_to_images(pdf_path, page)\n",
    "     quadrants = split_image_into_quadrants(image)\n",
    "# Save quadrants as PNG files\n",
    "     base_filename = Path(pdf_path).stem\n",
    "     quadrant_paths = save_quadrants_as_png(quadrants, output_dir, base_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f604a422-9323-439d-a9f7-bbf0047b5969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call Ollama LLMs. Encode PNGs too.\n",
    "def encode_png_file(png_path: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Encode a PNG file to base64\n",
    "    \n",
    "    Args:\n",
    "        png_path (str): Path to the PNG file\n",
    "    \n",
    "    Returns:\n",
    "        str: Base64 encoded image data, or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(png_path, 'rb') as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding {png_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_png_files(directory: str, pattern: str = \"*.png\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Find all PNG files in a directory\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Directory to search\n",
    "        pattern (str): File pattern (default: \"*.png\")\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: List of PNG file paths\n",
    "    \"\"\"\n",
    "    search_path = os.path.join(directory, pattern)\n",
    "    png_files = glob.glob(search_path)\n",
    "    png_files.sort()  # Sort for consistent ordering\n",
    "    return png_files\n",
    "\n",
    "def query_ollama(model, prompt, png_path):\n",
    "    if png_path != \"\": \n",
    "        encoded_image = encode_png_file(png_path)\n",
    "    \n",
    "        response = ollama.chat(\n",
    "           model=model,\n",
    "           messages=[{'role': 'user', 'content': prompt,'images': [encoded_image]}],\n",
    "           options={\n",
    "               'temperature': 0.7,  # Range: 0.0 to 1.0\n",
    "           }\n",
    "        )\n",
    "    else:\n",
    "        response = ollama.chat(\n",
    "           model=model,\n",
    "           messages=[{'role': 'user', 'content': prompt}],\n",
    "           options={\n",
    "               'temperature': 0.7,  # Range: 0.0 to 1.0\n",
    "           }\n",
    "        )\n",
    "\n",
    "    foo = response.message.content[8:-4]\n",
    "    print(response.message.content)\n",
    "    response_J = json.loads(foo)\n",
    "    return response_J\n",
    "\n",
    "def query_ollama_multiple(model, prompt, png_path): #same as above, except png_path is a list of pngs\n",
    "    #!!!!!This does not work. Right now ollama does not support multiple images\n",
    "    encoded_images = []\n",
    "    \n",
    "    for i,png in enumerate(png_path):\n",
    "       encoded_images.append(encode_png_file(png))\n",
    "   \n",
    "    response = ollama.chat(\n",
    "       model=model,\n",
    "       messages=[{'role': 'user', 'content': prompt,'images': [encoded_images]}],\n",
    "       options={\n",
    "           'temperature': 0.7,  # Range: 0.0 to 1.0\n",
    "       }\n",
    "    )\n",
    "\n",
    "    foo = response.message.content[8:-4]\n",
    "    response_J = json.loads(foo)\n",
    "    return response_J    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "034780ff-bc32-417f-9c1e-35a47ea5a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Claude\n",
    "Claude = \"claude-sonnet-4-20250514\"\n",
    "\n",
    "#initialize\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=key  # or set ANTHROPIC_API_KEY environment variable\n",
    ")\n",
    "\n",
    "def claude_simple(prompt_text):\n",
    "    content = []\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": prompt_text\n",
    "    })\n",
    "    \n",
    "    message = client.messages.create(\n",
    "        model=Claude,\n",
    "        max_tokens=2000,\n",
    "        temperature = 0.5,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content\n",
    "        }]\n",
    "    )\n",
    "    return message.content[0].text \n",
    "\n",
    "def upload_multiple_images_to_claude(image_paths, prompt_text):\n",
    "    \"\"\"\n",
    "    Upload multiple images to Claude\n",
    "    \n",
    "    Args:\n",
    "        image_paths (list): List of paths to image files\n",
    "        prompt_text (str): Text prompt to send along with the images\n",
    "    \n",
    "    Returns:\n",
    "        str: Claude's response\n",
    "    \"\"\"\n",
    "    \n",
    "    content = []\n",
    "    \n",
    "    # Add all images\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        image_path = Path(image_path)\n",
    "        \n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "        \n",
    "        content.append({\n",
    "            \"type\": \"image\",\n",
    "            \"source\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": \"image/png\",\n",
    "                \"data\": image_data\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Add text prompt\n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": prompt_text\n",
    "    })\n",
    "    \n",
    "    message = client.messages.create(\n",
    "        model=Claude,\n",
    "        max_tokens=2000,\n",
    "        temperature = 0.5,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daefdd6-baa8-47cc-9510-df6afd5d7749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping YOLO predictions\n",
    "def crop_predictions(png_file_path, predictions_csv_path, scaling_factor=1.0, output_dir=\"cropped_images\"):\n",
    "    \"\"\"\n",
    "    Crop image regions based on predictions CSV with bounding box coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    png_file_path (str): Path to the input PNG image\n",
    "    predictions_csv_path (str): Path to the predictions CSV file\n",
    "    scaling_factor (float): Factor to scale the bounding box size (1.0 = original size)\n",
    "    output_dir (str): Directory to save cropped images\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(predictions_csv_path)\n",
    "    \n",
    "    # Load the image\n",
    "    image = Image.open(png_file_path)\n",
    "    image_width, image_height = image.size\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get the base name of the PNG file (without extension)\n",
    "    base_name = Path(png_file_path).stem\n",
    "    \n",
    "    cropped_images = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Parse the xyxy coordinates from string representation\n",
    "        # Remove 'tensor()' wrapper and convert to float\n",
    "        coords_str = row['Xyxy'].strip('[]')\n",
    "        coords_parts = coords_str.split(', ')\n",
    "        \n",
    "        # Extract numerical values from tensor format\n",
    "        coords = []\n",
    "        for part in coords_parts:\n",
    "            # Remove 'tensor(' and ')' and convert to float\n",
    "            num_str = part.strip().replace('tensor(', '').replace(')', '').replace('.', '')\n",
    "            coords.append(float(num_str))\n",
    "        \n",
    "        x1, y1, x2, y2 = coords\n",
    "        \n",
    "        # Calculate center and dimensions of bounding box\n",
    "        center_x = (x1 + x2) / 2\n",
    "        center_y = (y1 + y2) / 2\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        \n",
    "        # Apply scaling factor\n",
    "        scaled_width = width * scaling_factor\n",
    "        scaled_height = height * scaling_factor\n",
    "        \n",
    "        # Calculate new coordinates\n",
    "        new_x1 = center_x - scaled_width / 2\n",
    "        new_y1 = center_y - scaled_height / 2\n",
    "        new_x2 = center_x + scaled_width / 2\n",
    "        new_y2 = center_y + scaled_height / 2\n",
    "        \n",
    "        # Ensure coordinates are within image bounds\n",
    "        new_x1 = max(0, min(new_x1, image_width))\n",
    "        new_y1 = max(0, min(new_y1, image_height))\n",
    "        new_x2 = max(0, min(new_x2, image_width))\n",
    "        new_y2 = max(0, min(new_y2, image_height))\n",
    "        \n",
    "        # Crop the image\n",
    "        cropped_img = image.crop((new_x1, new_y1, new_x2, new_y2))\n",
    "        \n",
    "        # Create filename for cropped image\n",
    "        prediction_clean = row['Prediction'].replace('/', '_').replace(' ', '_')\n",
    "        confidence = f\"{row['Confidence']:.2f}\"\n",
    "        output_filename = f\"{base_name}_{idx:03d}_{prediction_clean}_conf{confidence}.png\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "        # Save cropped image\n",
    "        cropped_img.save(output_path)\n",
    "        \n",
    "        cropped_images.append({\n",
    "            'index': idx,\n",
    "            'prediction': row['Prediction'],\n",
    "            'confidence': row['Confidence'],\n",
    "            'original_coords': (x1, y1, x2, y2),\n",
    "            'scaled_coords': (new_x1, new_y1, new_x2, new_y2),\n",
    "            'output_path': output_path,\n",
    "            'size': cropped_img.size\n",
    "        })\n",
    "        \n",
    "        print(f\"Saved: {output_filename} ({cropped_img.size[0]}x{cropped_img.size[1]})\")\n",
    "    \n",
    "    print(f\"\\nProcessed {len(cropped_images)} predictions\")\n",
    "    print(f\"Cropped images saved to: {output_dir}\")\n",
    "    \n",
    "    return cropped_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f2199-8ea3-4bcb-b5fb-89ce326bde1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valve crop and look up \n",
    "def lookup_valve(valves, png_file_path, predictions_csv_path, scaling_factor=1.0, output_dir=\"cropped_images\"):\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(predictions_csv_path)\n",
    "    \n",
    "    # Load the image\n",
    "    image = Image.open(png_file_path)\n",
    "    image_width, image_height = image.size\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get the base name of the PNG file (without extension)\n",
    "    base_name = Path(png_file_path).stem\n",
    "    \n",
    "    cropped_images = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Parse the xyxy coordinates from string representation\n",
    "        # Remove 'tensor()' wrapper and convert to float\n",
    "        coords_str = row['Xyxy'].strip('[]')\n",
    "        coords_parts = coords_str.split(', ')\n",
    "        prediction = row['Prediction']\n",
    "        \n",
    "        \n",
    "        # Extract numerical values from tensor format\n",
    "        coords = []\n",
    "        for part in coords_parts:\n",
    "            # Remove 'tensor(' and ')' and convert to float\n",
    "            num_str = part.strip().replace('tensor(', '').replace(')', '').replace('.', '')\n",
    "            coords.append(float(num_str))\n",
    "        \n",
    "        x1, y1, x2, y2 = coords\n",
    "        \n",
    "        # Calculate center and dimensions of bounding box\n",
    "        center_x = (x1 + x2) / 2\n",
    "        center_y = (y1 + y2) / 2\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        \n",
    "        # Apply scaling factor\n",
    "        scaled_width = width * scaling_factor\n",
    "        scaled_height = height * scaling_factor\n",
    "        \n",
    "        # Calculate new coordinates\n",
    "        new_x1 = center_x - scaled_width / 2\n",
    "        new_y1 = center_y - scaled_height / 2\n",
    "        new_x2 = center_x + scaled_width / 2\n",
    "        new_y2 = center_y + scaled_height / 2\n",
    "        \n",
    "        # Ensure coordinates are within image bounds\n",
    "        new_x1 = max(0, min(new_x1, image_width))\n",
    "        new_y1 = max(0, min(new_y1, image_height))\n",
    "        new_x2 = max(0, min(new_x2, image_width))\n",
    "        new_y2 = max(0, min(new_y2, image_height))\n",
    "        \n",
    "        # Crop the image\n",
    "        cropped_img = image.crop((new_x1, new_y1, new_x2, new_y2))\n",
    "        \n",
    "        # Create filename for cropped image\n",
    "        prediction_clean = row['Prediction'].replace('/', '_').replace(' ', '_')\n",
    "        confidence = f\"{row['Confidence']:.2f}\"\n",
    "        output_filename = f\"{base_name}_{idx:03d}_{prediction_clean}_conf{confidence}.png\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "        # Save cropped image\n",
    "        cropped_img.save(output_path)\n",
    "        \n",
    "        cropped_images.append({\n",
    "            'index': idx,\n",
    "            'prediction': row['Prediction'],\n",
    "            'confidence': row['Confidence'],\n",
    "            'original_coords': (x1, y1, x2, y2),\n",
    "            'scaled_coords': (new_x1, new_y1, new_x2, new_y2),\n",
    "            'output_path': output_path,\n",
    "            'size': cropped_img.size\n",
    "        })\n",
    "        \n",
    "        print(f\"Saved: {output_filename} ({cropped_img.size[0]}x{cropped_img.size[1]})\")\n",
    "    \n",
    "    print(f\"\\nProcessed {len(cropped_images)} predictions\")\n",
    "    print(f\"Cropped images saved to: {output_dir}\")\n",
    "    \n",
    "    return cropped_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5e59e-8b38-4230-b326-b86306b87986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#main\n",
    "pdf_file = \"/home/peter/test3.pdf\"\n",
    "page = 0\n",
    "ollama_model = 'gemma3:27b'\n",
    "\n",
    "output_dir = pdf_file[:-4]\n",
    "filename = os.path.splitext(os.path.basename(pdf_file))[0]\n",
    "create_quads(pdf_file,page,output_dir) #create PNG files\n",
    "\n",
    "# Call Gemma3:27b via Ollama to get metadata\n",
    "prompt = \"\"\"List the project name, company name, drawing (DWG) name, drawing number, location, date, revision, author, \n",
    "and any other information found in the title block in the lower right corner of the image. Return data as JSON.\"\"\"\n",
    "png_path = output_dir+\"/\"+filename+\"_q4.png\"\n",
    "drawing_info = query_ollama(ollama_model, prompt, png_path\n",
    "                           )\n",
    "# Call Claude and get line list. Need to send all 4 quadrants\n",
    "full_dwg = [] #all 4 quadrants\n",
    "for i in range(4):\n",
    "    n = i+1\n",
    "    full_dwg.append(output_dir+\"/\"+filename+\"_q\"+str(n)+\".png\")\n",
    "prompt = \"\"\"list the pipe lines on this P&ID with the line number, primary size, and what equipment, off-page connectors, \n",
    "or other lines it is connected to. Return the data as JSON and only return JSON.\"\"\"\n",
    "result = upload_multiple_images_to_claude(full_dwg, prompt)\n",
    "line_list = json.loads(result[8:-4])\n",
    "\n",
    "#equipment list\n",
    "prompt = \"\"\"List the equipment on this P&ID with the name, description and associated metadata. Only list the primary equipment. \n",
    "Do not list piping, valves, instruments or information about the drawing itself. Return data as JSON\"\"\"\n",
    "result = upload_multiple_images_to_claude(full_dwg, prompt)\n",
    "equip_list = json.loads(result[8:-4])\n",
    "\n",
    "#valves\n",
    "valves = []\n",
    "#for each quadrant\n",
    "for quad in range(4):\n",
    "    #clean up previous runs\n",
    "    directory_path = \"/home/peter/Desktop/yolov5/runs/detect\"\n",
    "    shutil.rmtree(directory_path)\n",
    "    #image size is important - if the size is too far off, detection is not accurate\n",
    "    !python \"./detect.py\" --weights \"./best.pt\" --source full_dwg[quad] --conf-thres 0.5 --save-csv --imgsz 5000\n",
    "    result_path = directory_path+\"/exp\"\n",
    "    #create cropped images\n",
    "    crop_predictions(full_dwg[quad], result_path, scaling_factor=10.0, output_dir=results_path)\n",
    "    #go through each detected item and add the ones we want to the valve list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d469be14-448c-42d3-b2b2-f10c124ee351",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/peter/Desktop/yolov5\n",
    "!python3 --version\n",
    "!source .venv/bin/activate\n",
    "!python3 \"detect.py\" --weights \"./best.pt\" --source \"/home/peter/test3/test3_q1.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb776695-f642-4b53-93b2-12e5c6db9278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a108a-5409-4b9e-83d1-17d489fa5a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7080921-3627-409a-a0f0-886dc0790bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remember to clean up png files when done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
